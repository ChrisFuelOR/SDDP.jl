<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Create a general policy graph · SDDP.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link href="../../assets/documenter.css" rel="stylesheet" type="text/css"/><link href="../../logo.ico" rel="icon" type="image/x-icon"/></head><body><nav class="toc"><a href="../../"><img class="logo" src="../../assets/logo.png" alt="SDDP.jl logo"/></a><h1>SDDP.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../../">Home</a></li><li><span class="toctext">How-to guides</span><ul><li><a class="toctext" href="../add_a_multidimensional_state_variable/">Add a multi-dimensional state variable</a></li><li><a class="toctext" href="../add_a_risk_measure/">Add a risk measure</a></li><li><a class="toctext" href="../add_multidimensional_noise_Terms/">Add multi-dimensional noise terms</a></li><li><a class="toctext" href="../add_noise_in_the_constraint_matrix/">Add noise in the constraint matrix</a></li><li><a class="toctext" href="../choose_a_stopping_rule/">Choose a stopping rule</a></li><li class="current"><a class="toctext" href>Create a general policy graph</a><ul class="internal"><li><a class="toctext" href="#Creating-a-[SDDP.Graph](@ref)-1">Creating a <code>SDDP.Graph</code></a></li><li><a class="toctext" href="#Creating-a-policy-graph-1">Creating a policy graph</a></li></ul></li><li><a class="toctext" href="../debug_a_model/">Debug a model</a></li><li><a class="toctext" href="../improve_computational_performance/">Improve computational performance</a></li><li><a class="toctext" href="../simulate_using_a_different_sampling_scheme/">Simulate using a different sampling scheme</a></li><li><a class="toctext" href="../upgrade_from_the_old_sddp/">Upgrade from the old SDDP.jl</a></li></ul></li><li><span class="toctext">Tutorials</span><ul><li><a class="toctext" href="../../tutorial/01_first_steps/">Basic I: first steps</a></li><li><a class="toctext" href="../../tutorial/02_adding_uncertainty/">Basic II: adding uncertainty</a></li><li><a class="toctext" href="../../tutorial/03_objective_uncertainty/">Basic III: objective uncertainty</a></li><li><a class="toctext" href="../../tutorial/04_markov_uncertainty/">Basic IV: Markov uncertainty</a></li><li><a class="toctext" href="../../tutorial/05_plotting/">Basic V: plotting</a></li><li><a class="toctext" href="../../tutorial/06_warnings/">Basic VI: words of warning</a></li><li><a class="toctext" href="../../tutorial/11_objective_states/">Advanced I: objective states</a></li><li><a class="toctext" href="../../tutorial/12_belief_states/">Advanced II: belief states</a></li><li><a class="toctext" href="../../tutorial/13_integrality/">Advanced III: integrality</a></li></ul></li><li><span class="toctext">Examples</span><ul><li><a class="toctext" href="../../examples/the_farmers_problem/">The farmer&#39;s problem</a></li></ul></li><li><a class="toctext" href="../../apireference/">Reference</a></li></ul></nav><article id="docs"><header><nav><ul><li>How-to guides</li><li><a href>Create a general policy graph</a></li></ul><a class="edit-page" href="https://github.com/odow/SDDP.jl/blob/master/docs/src/guides/create_a_general_policy_graph.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Create a general policy graph</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Create-a-general-policy-graph-1" href="#Create-a-general-policy-graph-1">Create a general policy graph</a></h1><p>SDDP.jl uses the concept of a <em>policy graph</em> to formulate multistage stochastic programming problems. We <em>highly</em> recommend that you read the following paper before continuing with this tutorial.</p><ul><li>Dowson, O. (2018). The policy graph decomposition of multistage stochastic optimization problems. Optimization Online. <a href="http://www.optimization-online.org/DB_HTML/2018/11/6914.html">link</a></li></ul><h2><a class="nav-anchor" id="Creating-a-[SDDP.Graph](@ref)-1" href="#Creating-a-[SDDP.Graph](@ref)-1">Creating a <a href="../../apireference/#SDDP.Graph"><code>SDDP.Graph</code></a></a></h2><h3><a class="nav-anchor" id="Linear-graphs-1" href="#Linear-graphs-1">Linear graphs</a></h3><p>Linear policy graphs can be created using the <a href="../../apireference/#SDDP.LinearGraph"><code>SDDP.LinearGraph</code></a> function.</p><pre><code class="language-julia-repl">julia&gt; graph = SDDP.LinearGraph(3)
Root
 0
Nodes
 1
 2
 3
Arcs
 0 =&gt; 1 w.p. 1.0
 1 =&gt; 2 w.p. 1.0
 2 =&gt; 3 w.p. 1.0</code></pre><p>We can add nodes to a graph using <a href="../../apireference/#SDDP.add_node"><code>SDDP.add_node</code></a> and edges using <a href="../../apireference/#SDDP.add_edge"><code>SDDP.add_edge</code></a>.</p><pre><code class="language-julia-repl">julia&gt; SDDP.add_node(graph, 4)

julia&gt; SDDP.add_edge(graph, 3 =&gt; 4, 1.0)

julia&gt; SDDP.add_edge(graph, 4 =&gt; 1, 0.9)

julia&gt; graph
Root
 0
Nodes
 1
 2
 3
 4
Arcs
 0 =&gt; 1 w.p. 1.0
 1 =&gt; 2 w.p. 1.0
 2 =&gt; 3 w.p. 1.0
 3 =&gt; 4 w.p. 1.0
 4 =&gt; 1 w.p. 0.9</code></pre><p>Look! We just made a cyclic graph! SDDP.jl can solve infinite horizon problems. The probability on the arc that completes a cycle should be interpreted as a discount factor.</p><h3><a class="nav-anchor" id="Markovian-policy-graphs-1" href="#Markovian-policy-graphs-1">Markovian policy graphs</a></h3><p>Markovian policy graphs can be created using the <a href="../../apireference/#SDDP.MarkovianGraph"><code>SDDP.MarkovianGraph</code></a> function.</p><pre><code class="language-julia-repl">julia&gt; SDDP.MarkovianGraph(Matrix{Float64}[[1.0]&#39;, [0.4 0.6]])
Root
 (0, 1)
Nodes
 (1, 1)
 (2, 1)
 (2, 2)
Arcs
 (0, 1) =&gt; (1, 1) w.p. 1.0
 (1, 1) =&gt; (2, 1) w.p. 0.4
 (1, 1) =&gt; (2, 2) w.p. 0.6</code></pre><h3><a class="nav-anchor" id="General-graphs-1" href="#General-graphs-1">General graphs</a></h3><p>Arbitrarily complicated graphs can be constructed using <a href="../../apireference/#SDDP.Graph"><code>SDDP.Graph</code></a>, <a href="../../apireference/#SDDP.add_node"><code>SDDP.add_node</code></a> and <a href="../../apireference/#SDDP.add_edge"><code>SDDP.add_edge</code></a>. For example</p><pre><code class="language-julia-repl">julia&gt; graph = SDDP.Graph(:root_node)
Root
 root_node
Nodes
Arcs

julia&gt; SDDP.add_node(graph, :decision_node)

julia&gt; SDDP.add_edge(graph, :root_node =&gt; :decision_node, 1.0)

julia&gt; SDDP.add_edge(graph, :decision_node =&gt; :decision_node, 0.9)

julia&gt; graph
Root
 root_node
Nodes
 decision_node
Arcs
 root_node =&gt; decision_node w.p. 1.0
 decision_node =&gt; decision_node w.p. 0.9</code></pre><h2><a class="nav-anchor" id="Creating-a-policy-graph-1" href="#Creating-a-policy-graph-1">Creating a policy graph</a></h2><p>Once you have constructed an instance of [<code>SDDP.Graph</code>], you can create a policy graph by passing the graph as the first argument.</p><pre><code class="language-julia-repl">julia&gt; graph = SDDP.Graph(
           :root_node,
           [:decision_node],
           [
               (:root_node =&gt; :decision_node, 1.0),
               (:decision_node =&gt; :decision_node, 0.9)
           ]);

julia&gt; model = SDDP.PolicyGraph(
               graph,
               lower_bound = 0,
               optimizer = with_optimizer(GLPK.Optimizer)) do subproblem, node
           println(&quot;Called from node: &quot;, node)
       end;
Called from node: decision_node</code></pre><h3><a class="nav-anchor" id="Special-cases-1" href="#Special-cases-1">Special cases</a></h3><p>There are two special cases which cover the majority of models in the literature.</p><ul><li><p><a href="../../apireference/#SDDP.LinearPolicyGraph"><code>SDDP.LinearPolicyGraph</code></a> is a special case where a <a href="../../apireference/#SDDP.LinearGraph"><code>SDDP.LinearGraph</code></a> is passed as the first argument.</p></li><li><p><a href="../../apireference/#SDDP.MarkovianPolicyGraph"><code>SDDP.MarkovianPolicyGraph</code></a> is a special case where a <a href="../../apireference/#SDDP.MarkovianGraph"><code>SDDP.MarkovianGraph</code></a> is passed as the first argument.</p></li></ul><p>Note that the type of the names of all nodes (including the root node) must be the same. In this case, they are <code>Symbol</code>s.</p><footer><hr/><a class="previous" href="../choose_a_stopping_rule/"><span class="direction">Previous</span><span class="title">Choose a stopping rule</span></a><a class="next" href="../debug_a_model/"><span class="direction">Next</span><span class="title">Debug a model</span></a></footer></article></body></html>
