<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Manual · SDDP.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/><link href="assets/custom.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>SDDP.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" action="search.html"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li class="current"><a class="toctext" href="index.html">Manual</a><ul class="internal"><li><a class="toctext" href="#Overview-1">Overview</a></li><li><a class="toctext" href="#Getting-started-1">Getting started</a></li><li><a class="toctext" href="#Formulating-the-problem-1">Formulating the problem</a></li><li><a class="toctext" href="#Communicating-the-problem-to-the-solver-1">Communicating the problem to the solver</a></li><li><a class="toctext" href="#Solving-the-problem-efficiently-1">Solving the problem efficiently</a></li><li><a class="toctext" href="#Understanding-the-solution-1">Understanding the solution</a></li><li><a class="toctext" href="#Extras-for-experts-1">Extras for experts</a></li></ul></li><li><a class="toctext" href="readings.html">Readings</a></li><li><a class="toctext" href="apireference.html">Reference</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href="index.html">Manual</a></li></ul><a class="edit-page" href="https://github.com/odow/SDDP.jl/tree/39f13029b5bb7b8b2c1ed5d8b7f7ec0f266181ee/docs/src/index.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Manual</span><a class="fa fa-bars" href="#"></a></div></header><h2><a class="nav-anchor" id="Overview-1" href="#Overview-1">Overview</a></h2><p><em>SDDP.jl - Stochastic Dual Dynamic Programming in Julia.</em></p><p>SDDP.jl is a package for solving large multi-stage convex stocastic optimization problems. In this manual, we&#39;re going to assume a reasonable amount of background knowledge about stochastic optimization, the SDDP algorithm, Julia, and JuMP.</p><div class="admonition note"><div class="admonition-title">Note</div><div class="admonition-text"><p>If you don&#39;t have that background, you may want to brush up on some <a href="readings.html">reading material</a>.</p></div></div><h3><a class="nav-anchor" id="Types-of-problems-SDDP.jl-can-solve-1" href="#Types-of-problems-SDDP.jl-can-solve-1">Types of problems SDDP.jl can solve</a></h3><p>To start, lets discuss the types of problems SDDP.jl can solve, since it has a few features that are non-standard, and it is missing some features that are standard.</p><p>SDDP.jl can solve multi-stage convex stochastic optimizations problems with</p><ul><li><p>a finite discrete number of states;</p></li><li><p>continuous state and control variables;</p></li><li><p>Hazard-Decision (Wait-and-See) uncertainty realization;</p></li><li><p>stagewise independent uncertainty in the RHS of the constraints that is  drawn from a finite discrete distribution;</p></li><li><p>stagewise independent uncertainty in the objective function that is  drawn from a finite discrete distribution;</p></li><li><p>a markov chain for temporal dependence. The markov chain forms a directed,  acyclic, feed-forward graph with a finite (and at least one) number of  markov states in each stage.</p></li></ul><div class="admonition note"><div class="admonition-title">Note</div><div class="admonition-text"><p>Stagewise independent uncertainty in the constraint coefficients is <strong>not</strong> supported. You should reformulate the problem, or model the uncertainty as a markov chain.</p></div></div><p>In this manual, we detail the many features of SDDP.jl through the classic example of balancing a portfolio of stocks and bonds over time.</p><h2><a class="nav-anchor" id="Getting-started-1" href="#Getting-started-1">Getting started</a></h2><p>This package is unregistered so you will need to <code>Pkg.clone</code> it as follows:</p><pre><code class="language-julia">Pkg.clone(&quot;https://github.com/odow/SDDP.jl.git&quot;)</code></pre><p>If you want to use the parallel features of SDDP.jl, you should start Julia with some worker processes (<code>julia -p N</code>), or add by running <code>julia&gt; addprocs(N)</code> in a running Julia session.</p><h2><a class="nav-anchor" id="Formulating-the-problem-1" href="#Formulating-the-problem-1">Formulating the problem</a></h2><h3><a class="nav-anchor" id="The-Asset-Management-Problem-1" href="#The-Asset-Management-Problem-1">The Asset Management Problem</a></h3><p>The goal of the asset management problem is to choose an investment portfolio that is composed of stocks and bonds in order to meet a target wealth goal at the end of the time horizon. After five, and ten years, the agent observes the portfolio and is able to re-balance their wealth between the two asset classes. As an extension to the original problem, we introduce two new random variables. The first that represents a source of additional wealth in years 5 and 10. The second is an immediate reward that the agent incurs for holding stocks at the end of years 5 and 10. This can be though of as a dividend that cannot be reinvested.</p><h2><a class="nav-anchor" id="Communicating-the-problem-to-the-solver-1" href="#Communicating-the-problem-to-the-solver-1">Communicating the problem to the solver</a></h2><p>The second step in the optimization process is communicating the problem to the solver. To do this, we are going to build each subproblem as a JuMP model, and provide some metadata that describes how the JuMP subproblems inter-relate.</p><h3><a class="nav-anchor" id="The-Model-Constructor-1" href="#The-Model-Constructor-1">The Model Constructor</a></h3><p>The core type of SDDP.jl is the <code>SDDPModel</code> object. It can be constructed with</p><pre><code class="language-julia">m = SDDPModel( ... metadata as keyword arguments ... ) do sp, t, i
    ... JuMP subproblem definition ...
end</code></pre><p>We draw the readers attention to three sections in the SDDPModel constructor.</p><h4><a class="nav-anchor" id="do-sp,-t,-i-...-end-1" href="#do-sp,-t,-i-...-end-1">do sp, t, i ... end</a></h4><h4><a class="nav-anchor" id="Keyword-Metadata-1" href="#Keyword-Metadata-1">Keyword Metadata</a></h4><h4><a class="nav-anchor" id="JuMP-Subproblem-1" href="#JuMP-Subproblem-1">JuMP Subproblem</a></h4><h3><a class="nav-anchor" id="State-Variables-1" href="#State-Variables-1">State Variables</a></h3><p>We can define a new state variable in the stage problem <code>sp</code> using the <code>@state</code> macro:</p><pre><code class="language-julia">@state(sp, x &gt;= 0.5, x0==1)</code></pre><p>The second argument (<code>x</code>) refers to the outgoing state variable (i.e. the value at the end of the stage). The third argument (<code>x0</code>) refers to the incoming state variable (i.e. the value at the beginning of the stage). For users familiar with SDDP, SDDP.jl handles all the calculation of the dual variables needed to evaluate the cuts automatically behind the scenes.</p><p>The <code>@state</code> macro is just short-hand for writing:</p><pre><code class="language-julia">@variable(sp, x &gt;= 0.5)
@variable(sp, x0, start=1)
SDDP.statevariable!(sp, x0, x)</code></pre><p>This illustrates how we can use indexing just as we would in a JuMP <code>@variable</code> macro:</p><pre><code class="language-julia">X0 = [3.0, 2.0]
@state(sp, x[i=1:2], x0==X0[i])</code></pre><p>In this case, both <code>x</code> and <code>x0</code> are JuMP dicts that can be indexed with the keys <code>1</code> and <code>2</code>. All the indices must be specified in the second argument, but they can be referred to in the third argument. The indexing of <code>x0</code> will be identical to that of <code>x.</code></p><p>There is also a plural version of the <code>@state</code> macro:</p><pre><code class="language-julia">@states(sp, begin
    x &gt;= 0.0, x0==1
    y &gt;= 0.0, y0==1
end)</code></pre><h3><a class="nav-anchor" id="Standard-JuMP-machinery-1" href="#Standard-JuMP-machinery-1">Standard JuMP machinery</a></h3><p>Remember that <code>sp</code> is just a normal JuMP model, and so (almost) anything you can do in JuMP, you can do in SDDP.jl. The one exception is the objective, which we detail in the next section.</p><p>However,, control variables are just normal JuMP variables and can be created using <code>@variable</code> or <code>@variables</code>. Dynamical constraints, and feasiblity sets can be specified using <code>@constraint</code> or <code>@constraints</code>.</p><h3><a class="nav-anchor" id="The-stage-objective-1" href="#The-stage-objective-1">The stage objective</a></h3><p>If there is no stagewise independent uncertainty in the objective, then the stage objective (i.e. ignoring the future cost) can be set via the <code>@stageobjective</code> macro. This is similar to the JuMP <code>@objective</code> macro, but without the sense argument. For example:</p><pre><code class="language-julia">@stageobjective(sp, obj)</code></pre><p>If there is stagewise independent noise in the objective, we add an additional argument to <code>@stageobjective</code> that has the form <code>kw=realizations</code>.</p><p><code>kw</code> is a symbol that can appear anywhere in <code>obj</code>, and <code>realizations</code> is a vector of realizations of the uncertainty. For example:</p><pre><code class="language-julia">@stageobjective(sp, kw=realizations, obj)
setnoiseprobability!(sp, [0.2, 0.3, 0.5])</code></pre><p><code>setnoiseprobability!</code> can be used to specify the finite discrete distribution of the realizations (it must sum to 1.0). If you don&#39;t explicitly call <code>setnoiseprobability!</code>, the distribution is assumed to be uniform.</p><p>Other examples include:</p><pre><code class="language-julia"># noise is a coefficient
@stageobjective(sp, c=[1.0, 2.0, 3.0], c * x)
# noise is used to index a variable
@stageobjective(sp, i=[1,2,3], 2 * x[i])</code></pre><h3><a class="nav-anchor" id="Dynamics-with-Linear-Noise-1" href="#Dynamics-with-Linear-Noise-1">Dynamics with Linear Noise</a></h3><p>SDDP.jl also supports uncertainty in the right-hand-side of constraints. Instead of using the JuMP <code>@constraint</code> macro, we need to use the <code>@rhsnoise</code> macro:</p><pre><code class="language-julia">@rhsnoise(sp, w=[1,2,3], x &lt;= w)
setnoiseprobability!(sp, [0.2, 0.3, 0.5])</code></pre><p>Compared to <code>@constraint</code>, there are a couple of notable differences:</p><ul><li><p>indexing is <strong>not</strong> supported;</p></li><li><p>the second argument is a <code>kw=realizations</code> key-value pair like the <code>@stageobjective</code>;</p></li><li><p>the <code>kw</code> can on either side of the constraint as written, but when normalised  to an Ax &lt;= b form, it must only appear in the b vector.</p></li></ul><p>Multiple <code>@rhsnoise</code> constraints can be added, however they must have an identical number of elements in the <code>realizations</code> vector.</p><p>For example, the following are invalid in SDDP:</p><pre><code class="language-julia"># noise appears as a variable coefficient
@rhsnoise(sp, w=[1,2,3], w * x &lt;= 1)

# JuMP style indexing
@rhsnoise(sp, w=[1,2,3], [i=1:10; mod(i, 2) == 0], x[i] &lt;= w)

# noises have different number of realizations
@rhsnoise(sp, w=[1,2,3], x &lt;= w)
@rhsnoise(sp, w=[2,3],   x &gt;= w-1)</code></pre><div class="admonition note"><div class="admonition-title">Note</div><div class="admonition-text"><p>Noises in the constraints are sampled with the noise in the objective. Therefore, there should be the same number of elements in the realizations for the stage objective, as there are in the constraint noise.</p></div></div><p>There is also a plural form of the <code>@rhsnoise</code> macro:</p><pre><code class="language-julia">@rhsnoises(sp, w=[1,2,3], begin
    x &lt;= w
    x &gt;= w-1
end)
setnoiseprobability!(sp, [0.2, 0.3, 0.5])</code></pre><h3><a class="nav-anchor" id="Asset-Management-Example-1" href="#Asset-Management-Example-1">Asset Management Example</a></h3><p>We now have all the information necessary to define the Asset Management example in SDDP.jl:</p><pre><code class="language-julia">using SDDP, JuMP, Clp

m = SDDPModel(
               # we are minimizing
                sense = :Min,
               # there are 4 stages
               stages = 4,
               # a large negative value
      objective_bound = -1000.0,
               # a MathOptBase compatible solver
               solver = ClpSolver(),
               # transition probabilities of the lattice
    markov_transition = Array{Float64, 2}[
                        [1.0]&#39;,
                        [0.5 0.5],
                        [0.5 0.5; 0.5 0.5],
                        [0.5 0.5; 0.5 0.5]
                    ],
               # risk measures for each stage
         risk_measure = [
                        Expectation(),
                        Expectation(),
                        NestedAVaR(lambda = 0.5, beta=0.5),
                        Expectation()
                    ]
                            ) do sp, t, i
    # Data used in the problem
    ωs = [1.25, 1.06]
    ωb = [1.14, 1.12]
    Φ  = [-1, 5]
    Ψ  = [0.02, 0.0]

    # state variables
    @states(sp, begin
        xs &gt;= 0, xsbar==0
        xb &gt;= 0, xbbar==0
    end)

    if t == 1 # we can switch based on the stage
        # a constraint without noise
        @constraint(sp, xs + xb == 55 + xsbar + xbbar)
        # an objective without noise
        @stageobjective(sp, 0)
    elseif t == 2 || t == 3
        # a constraint with noisein the RHS
        @rhsnoise(sp, φ=Φ, ωs[i] * xsbar + ωb[i] * xbbar + φ == xs + xb)
        # an objective with noise
        @stageobjective(sp, ψ = Ψ, -ψ * xs)
        # noise will be sampled as (Φ[1], Ψ[1]) w.p. 0.6, (Φ[2], Ψ[2]) w.p. 0.4
        setnoiseprobability!(sp, [0.6, 0.4])
    else # when t == 4
        # some control variables
        @variable(sp, u &gt;= 0)
        @variable(sp, v &gt;= 0)
        # dynamics constraint
        @constraint(sp, ωs[i] * xsbar + ωb[i] * xbbar + u - v == 80)
        # an objective without noise
        @stageobjective(sp, 4u - v)
    end
end</code></pre><h2><a class="nav-anchor" id="Solving-the-problem-efficiently-1" href="#Solving-the-problem-efficiently-1">Solving the problem efficiently</a></h2><h2><a class="nav-anchor" id="Understanding-the-solution-1" href="#Understanding-the-solution-1">Understanding the solution</a></h2><h2><a class="nav-anchor" id="Extras-for-experts-1" href="#Extras-for-experts-1">Extras for experts</a></h2><h3><a class="nav-anchor" id="New-risk-measures-1" href="#New-risk-measures-1">New risk measures</a></h3><p>SDDP.jl makes it easy to create new risk measures. First, create a new subtype of the abstract type <code>SDDP.AbstractRiskMeasure</code>:</p><pre><code class="language-julia">immutable MyNewRiskMeasure &lt;: SDDP.AbstractRiskMeasure
end</code></pre><p>Then, overload the method <code>SDDP.modifyprobability!</code> for your new type. <code>SDDP.modifyprobability!</code> has the following signature:</p><pre><code class="language-julia">SDDP.modifyprobability!(
        measure::AbstractRiskMeasure,
        riskadjusted_distribution,
        original_distribution::Vector{Float64},
        observations::Vector{Float64},
        m::SDDPModel,
        sp::JuMP.Model
)</code></pre><p>where <code>original_distribution</code> contains the risk netural probability of each outcome in <code>observations</code> occurring (so that the probability of <code>observations[i]</code> is <code>original_distribution[i]</code>). The method should modify (in-place) the elements of <code>riskadjusted_distribution</code> to represent the risk-adjusted probabilities of the distribution.</p><p>To illustrate this, we shall define the worst-case riskmeasure (which places all the probability on the worst outcome):</p><pre><code class="language-julia">immutable WorstCase &lt;: SDDP.AbstractRiskMeasure end
function SDDP.modifyprobability!(measure::WorstCase,
        riskadjusted_distribution,
        original_distribution::Vector{Float64},
        observations::Vector{Float64},
        m::SDDPModel,
        sp::JuMP.Model
    )
    if JuMP.getobjectivesense(sp) == :Min
        # if minimizing, worst is the maximum outcome
        idx = indmax(observations)
    else
        # if maximizing, worst is the minimum outcome
        idx = indmin(observations)
    end
    # zero all probabilities
    riskadjusted_distribution .= 0.0
    # set worst to 1.0
    riskadjusted_distribution[idx] = 1.0
    # return
    return nothing
end</code></pre><p>The risk measure <code>WorstCase()</code> can now be used in any SDDP model.</p><div class="admonition note"><div class="admonition-title">Note</div><div class="admonition-text"></div></div><h3><a class="nav-anchor" id="New-cut-oracles-1" href="#New-cut-oracles-1">New cut oracles</a></h3><footer><hr/><a class="next" href="readings.html"><span class="direction">Next</span><span class="title">Readings</span></a></footer></article></body></html>
