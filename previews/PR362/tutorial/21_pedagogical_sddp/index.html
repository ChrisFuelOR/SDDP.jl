<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Expert I: pedagogical SDDP · SDDP.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="SDDP.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit">SDDP.jl</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><input class="collapse-toggle" id="menuitem-2" type="checkbox" checked/><label class="tocitem" for="menuitem-2"><span class="docs-label">Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../01_first_steps/">Basic I: first steps</a></li><li><a class="tocitem" href="../02_adding_uncertainty/">Basic II: adding uncertainty</a></li><li><a class="tocitem" href="../03_objective_uncertainty/">Basic III: objective uncertainty</a></li><li><a class="tocitem" href="../04_markov_uncertainty/">Basic IV: Markov uncertainty</a></li><li><a class="tocitem" href="../05_plotting/">Basic V: plotting</a></li><li><a class="tocitem" href="../06_warnings/">Basic VI: words of warning</a></li><li><a class="tocitem" href="../11_objective_states/">Advanced I: objective states</a></li><li><a class="tocitem" href="../12_belief_states/">Advanced II: belief states</a></li><li><a class="tocitem" href="../13_integrality/">Advanced III: integrality</a></li><li class="is-active"><a class="tocitem" href>Expert I: pedagogical SDDP</a><ul class="internal"><li><a class="tocitem" href="#Preliminaries:-background-theory"><span>Preliminaries: background theory</span></a></li><li><a class="tocitem" href="#Preliminaries:-Kelley&#39;s-cutting-plane-algorithm"><span>Preliminaries: Kelley&#39;s cutting plane algorithm</span></a></li><li><a class="tocitem" href="#Preliminaries:-approximating-the-cost-to-go-term"><span>Preliminaries: approximating the cost-to-go term</span></a></li><li><a class="tocitem" href="#Implementation:-modeling"><span>Implementation: modeling</span></a></li><li><a class="tocitem" href="#Implementation:-solution-algorithm"><span>Implementation: solution algorithm</span></a></li><li><a class="tocitem" href="#Implementation:-the-forward-pass"><span>Implementation: the forward pass</span></a></li><li><a class="tocitem" href="#Implementation:-the-backward-pass"><span>Implementation: the backward pass</span></a></li><li><a class="tocitem" href="#Implementation:-the-training-loop"><span>Implementation: the training loop</span></a></li><li><a class="tocitem" href="#Implementation:-decision-rules"><span>Implementation: decision rules</span></a></li><li><a class="tocitem" href="#Example:-infinite-horizon"><span>Example: infinite horizon</span></a></li></ul></li></ul></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox"/><label class="tocitem" for="menuitem-3"><span class="docs-label">How-to guides</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../guides/add_a_multidimensional_state_variable/">Add a multi-dimensional state variable</a></li><li><a class="tocitem" href="../../guides/add_a_risk_measure/">Add a risk measure</a></li><li><a class="tocitem" href="../../guides/add_multidimensional_noise_Terms/">Add multi-dimensional noise terms</a></li><li><a class="tocitem" href="../../guides/add_noise_in_the_constraint_matrix/">Add noise in the constraint matrix</a></li><li><a class="tocitem" href="../../guides/choose_a_stopping_rule/">Choose a stopping rule</a></li><li><a class="tocitem" href="../../guides/create_a_general_policy_graph/">Create a general policy graph</a></li><li><a class="tocitem" href="../../guides/debug_a_model/">Debug a model</a></li><li><a class="tocitem" href="../../guides/improve_computational_performance/">Improve computational performance</a></li><li><a class="tocitem" href="../../guides/simulate_using_a_different_sampling_scheme/">Simulate using a different sampling scheme</a></li><li><a class="tocitem" href="../../guides/upgrade_from_the_old_sddp/">Upgrade from the old SDDP.jl</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">Examples</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../examples/simple_hydro_thermal/">Hydro-thermal scheduling</a></li><li><a class="tocitem" href="../../examples/the_farmers_problem/">The farmer&#39;s problem</a></li></ul></li><li><a class="tocitem" href="../../apireference/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href>Expert I: pedagogical SDDP</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Expert I: pedagogical SDDP</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/odow/SDDP.jl/blob/master/docs/src/tutorial/21_pedagogical_sddp.jl" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Expert-I:-pedagogical-SDDP"><a class="docs-heading-anchor" href="#Expert-I:-pedagogical-SDDP">Expert I: pedagogical SDDP</a><a id="Expert-I:-pedagogical-SDDP-1"></a><a class="docs-heading-anchor-permalink" href="#Expert-I:-pedagogical-SDDP" title="Permalink"></a></h1><p>In this tutorial we walk through a simplified implementation of stochastic dual dynamic programming to explain the key concepts.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>If you haven&#39;t already, go read <a href="../01_first_steps/#Basic-I:-first-steps">Basic I: first steps</a>, since it introduces much necessary background theory, and <a href="../02_adding_uncertainty/#Basic-II:-adding-uncertainty">Basic II: adding uncertainty</a>, since it defines the same example we will solve in our implementation.</p></div></div><p>For this implementation of SDDP, we&#39;re going to try and keep things simple. This is very much a &quot;vanilla&quot; version of SDDP; it doesn&#39;t have (m)any fancy computational tricks that you need to code a performant or stable version that will work on realistic instances. However, it will work on arbitrary policy graphs, including those with cycles such as infinite horizon problems!</p><p>In the interests of brevity, we will also include minimal error checking. Think about all the different ways you could break the code!</p><p>This tutorial uses the following packages. For clarity, we call <code>import PackageName</code> so that we must prefix <code>PackageName.</code> to all functions and structs provided by that package. Everything not prefixed is either part of base Julia, or we wrote it.</p><pre><code class="language-julia">import ForwardDiff
import GLPK
import JuMP
import Statistics</code></pre><h2 id="Preliminaries:-background-theory"><a class="docs-heading-anchor" href="#Preliminaries:-background-theory">Preliminaries: background theory</a><a id="Preliminaries:-background-theory-1"></a><a class="docs-heading-anchor-permalink" href="#Preliminaries:-background-theory" title="Permalink"></a></h2><div class="admonition is-success"><header class="admonition-header">Tip</header><div class="admonition-body"><p>This section is copied verbatim from <a href="../01_first_steps/#Basic-I:-first-steps">Basic I: first steps</a>. If it&#39;s familiar, skip to <a href="#Preliminaries:-Kelley&#39;s-cutting-plane-algorithm">Preliminaries: Kelley&#39;s cutting plane algorithm</a>.</p></div></div><p>Multistage stochastic programming is complicated, and the literature has not settled upon standard naming conventions, so we must begin with some unavoidable theory and notation.</p><p>A multistage stochastic program can be modeled by a <strong>policy graph</strong>. A policy graph is a graph with nodes and arcs. The simplest type of policy graph is a linear graph. Here&#39;s a linear graph with three nodes:</p><p><img src="../../assets/deterministic_linear_policy_graph.png" alt="Linear policy graph"/></p><p>In addition to nodes 1, 2, and 3, there is also a root node (0), and three arcs. Each arc has an origin node and a destination node, like <code>0 =&gt; 1</code>, and a corresponding probability of transitioning from the origin to the destination. For now, we can forget about the arc probabilities, because they are all 1.0.</p><p>We denote the set of nodes by <span>$\mathcal{N}$</span>, the root node by <span>$R$</span>, and the probability of transitioning from node <span>$i$</span> to node <span>$j$</span> by <span>$p_{ij}$</span>. (If no arc exists, then <span>$p_{ij} = 0$</span>). We define the set of successors of node <span>$i$</span> as <span>$i^+ = \{j \in N | P(i =&gt; j) &gt; 0\}$</span>.</p><p>Each square node in the graph corresponds to a place at which the agent makes a decision, and we call moments in time at which the agent makes a decision <strong>stages</strong>. By convention, we try to draw policy graphs from left-to-right, with the stages as columns. (There can be more than one node in a stage! We will see such examples in future tutorials when our graph has rows as well as columns.)</p><p>A common feature of multistage stochastic optimization problems is that they model an agent controlling a system over time. This system can be described by three types of variables.</p><ol><li><p><strong>State</strong> variables track a property of the system over time.</p><p>Each node has an associated <em>incoming</em> state variable (the value of the state at the start of the node), and an <em>outgoing</em> state variable (the value of the state at the end of the node).</p><p>Examples of state variables include the volume of water in a reservoir, the number of units of inventory in a warehouse, or the spatial position of a moving vehicle.</p><p>Because state variables track the system over time, each node must have the same set of state variables.</p><p>We denote state variables by the letter <span>$x$</span> for the incoming state variable and <span>$x^\prime$</span> for the outgoing state variable.</p></li><li><p><strong>Control</strong> variables are actions taken (implicitly or explicitly) by the agent within a node which modify the state variables.</p><p>Examples of control variables include releases of water from the reservoir, sales or purchasing decisions, and acceleration or braking of the vehicle.</p><p>Control variables are local to a node <span>$i$</span>, and they can differ between nodes. For example, some control variables may be available within certain nodes.</p><p>We denote control variables by the letter <span>$u$</span>.</p></li><li><p><strong>Random</strong> variables are finite, discrete, exogenous random variables that the agent observes at the start of a node, before the control variables are decided.</p><p>Examples of random variables include rainfall inflow into a reservoir, probalistic perishing of inventory, and steering errors in a vehicle.</p><p>Random variables are local to a node <span>$i$</span>, and they can differ between nodes. For example, some nodes may have random variables, and some nodes may not.</p><p>We denote random variables by the Greek letter <span>$\omega$</span> and the sample space from which they are drawn by <span>$\Omega_i$</span>. The probability of sampling <span>$\omega$</span> is denoted <span>$p_{\omega}$</span> for simplicity.</p></li></ol><p>In a node <span>$i$</span>, the three variables are related by a <strong>transition function</strong>, which maps the incoming state, the controls, and the random variables to the outgoing state as follows: <span>$x^\prime = T_i(x, u, \omega)$</span>.</p><p>As a result of entering a node <span>$i$</span> with the incoming state <span>$x$</span>, observing random variable <span>$\omega$</span>, and choosing control <span>$u$</span>, the agent incurs a cost <span>$C_i(x, u, \omega)$</span>. (If the agent is a maximizer, this can be a profit, or a negative cost.) We call <span>$C_i$</span> the <strong>stage objective</strong>.</p><p>To choose their control variables in node <span>$i$</span>, the agent uses a <strong>decision</strong> <strong>rule</strong> <span>$u = \pi_i(x, \omega)$</span>, which is a function that maps the incoming state variable and observation of the random variable to a control <span>$u$</span>. This control must satisfy some feasibilty requirements <span>$u \in U_i(x, \omega)$</span>.</p><p>The set of decision rules, with one element for each node in the policy graph, is called a <strong>policy</strong>.</p><p>The goal of the agent is to find a policy that minimizes the expected cost of starting at the root node with some initial condition <span>$x_R$</span>, and proceeding from node to node along the probabilistic arcs until they reach a node with no outgoing arcs.</p><p class="math-container">\[\min_{\pi} \mathbb{E}_{i \in R^+, \omega \in \Omega_i}[V_i^\pi(x_R, \omega)]\]</p><p>where</p><p class="math-container">\[V_i^\pi(x, \omega) = C_i(x, u, \omega) + \mathbb{E}_{j \in i^+, \varphi \in \Omega_j}[V_j(x^\prime, \varphi)]\]</p><p>where <span>$u = \pi_i(x, \omega) \in U_i(x, \omega)$</span>, and <span>$x^\prime = T_i(x, u, \omega)$</span>.</p><p>The expectations are a bit complicated, but they are equivalent to:</p><p class="math-container">\[\mathbb{E}_{j \in i^+, \varphi \in \Omega_j}[V_j(x^\prime, \varphi)] = \sum\limits_{j \in i^+} p_{ij} \left[\sum\limits_{\varphi \in \Omega_j} p_{\varphi}\left[V_j(x^\prime, \varphi)\right]\right]\]</p><p>An optimal policy is the set of decision rules that the agent can use to make these decisions and achieve the smallest expected cost.</p><p>Often, computing the cost of a policy is intractable due to the large number of nodes or possible realizations of the random variables. Instead, we can evaluate the policy using a Monte Carlo simulation. Each replicate of the simulation starts at the root node and probabilistically walks along the arcs of the policy graph until it reaches a node with not outgoing arcs. The cost of a replicate is the sum of the costs incurred at each node that was visited.</p><h3 id="Dynamic-programming-and-subproblems"><a class="docs-heading-anchor" href="#Dynamic-programming-and-subproblems">Dynamic programming and subproblems</a><a id="Dynamic-programming-and-subproblems-1"></a><a class="docs-heading-anchor-permalink" href="#Dynamic-programming-and-subproblems" title="Permalink"></a></h3><p>Now that we have formulated our problem, we need some ways of computing optimal decision rules. One way is to just use a heuristic like &quot;choose a control randomly from the set of feasible controls.&quot; However, such a policy is unlikely to be optimal.</p><p>One way of obtaining an optimal policy is to use Bellman&#39;s principle of optimality, a.k.a Dynamic Programming, and define a recursive <strong>subproblem</strong> as follows:</p><p class="math-container">\[\begin{aligned}
V_i(x, \omega) = \min\limits_{\bar{x}, x^\prime, u} \;\; &amp; C_i(\bar{x}, u, \omega) + \mathbb{E}_{j \in i^+, \varphi \in \Omega_j}[V_j(x^\prime, \varphi)]\\
&amp; x^\prime = T_i(\bar{x}, u, \omega) \\
&amp; u \in U_i(\bar{x}, \omega) \\
&amp; \bar{x} = x
\end{aligned}\]</p><p>Our decision rule, <span>$\pi_i(x, \omega)$</span>, solves this optimization problem and returns a <span>$u^*$</span> corresponding to an optimal solution.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>We add <span>$\bar{x}$</span> as a decision variable, along with the fishing constraint <span>$\bar{x} = x$</span> for two reasons: it makes it obvious that formulating a problem with <span>$x \times u$</span> results in a bilinear program instead of a linear program, and it simplifies that internal algorithm that SDDP.jl uses to find an optimal policy.</p></div></div><p>These subproblems are very difficult to solve exactly, because they involve recursive optimization problems with lots of nested expectations.</p><p>Therefore, instead of solving them exactly, SDDP works by iteratively approximating the expectation term of each subproblem, which is also called the cost-to-go term. For now, you don&#39;t need to understand the details, we will explain how shortly.</p><p>The subproblem view of a multistage stochastic program is also important, because it provides a convienient way of communicating the different parts of the broader problem, and it is how we will communicate the problem to SDDP.jl. All we need to do is drop the cost-to-go term and fishing constraint, and define a new subproblem <code>SP</code> as:</p><p class="math-container">\[\begin{aligned}
\texttt{SP}_i(x, \omega) : \min\limits_{\bar{x}, x^\prime, u} \;\; &amp; C_i(\bar{x}, u, \omega) \\
&amp; x^\prime = T_i(\bar{x}, u, \omega) \\
&amp; u \in U_i(\bar{x}, \omega) \\
\end{aligned}\]</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>When we talk about formulating a <strong>subproblem</strong> with SDDP.jl, this is the formulation we mean.</p></div></div><p>We&#39;ve retained the transition function and uncertainty set because they help to motivate the different components of the subproblem. However, in general, the subproblem can be more general. A better (less restrictive) representation might be:</p><p class="math-container">\[\begin{aligned}
\texttt{SP}_i(x, \omega) : \min\limits_{\bar{x}, x^\prime, u} \;\; &amp; C_i(\bar{x}, x^\prime, u, \omega) \\
&amp; (\bar{x}, x^\prime, u) \in \mathcal{X}_i(\omega)
\end{aligned}\]</p><p>Note that the outgoing state variable can appear in the objective, and we can add constraints involving the incoming and outgoing state variables. It should be obvious how to map between the two representations.</p><h2 id="Preliminaries:-Kelley&#39;s-cutting-plane-algorithm"><a class="docs-heading-anchor" href="#Preliminaries:-Kelley&#39;s-cutting-plane-algorithm">Preliminaries: Kelley&#39;s cutting plane algorithm</a><a id="Preliminaries:-Kelley&#39;s-cutting-plane-algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#Preliminaries:-Kelley&#39;s-cutting-plane-algorithm" title="Permalink"></a></h2><p>Kelley&#39;s cutting plane algorithm is an iterative method for minimizing convex functions. Given a convex function <span>$f(x)$</span>, Kelley&#39;s constructs an under-approximation of the function at the minimum by a set of first-order Taylor series approximations (called <strong>cuts</strong>) constructed at a set of <span>$K$</span> points <span>$k = 1,\ldots,K$</span>:</p><p class="math-container">\[\begin{aligned}
f^K = \min\limits_{\theta \in \mathbb{R}, x \in \mathbb{R}^N} \;\; &amp; \theta\\
&amp; \theta \ge f(x_k) + \frac{df}{dx}\left(x_k\right)^\top (x - x_k),\quad k=1,\ldots,K\\
&amp; \theta \ge M,
\end{aligned}\]</p><p>where <span>$M$</span> is a sufficiently large negative number that is a lower bound for <span>$f$</span> over the domain of <span>$x$</span>.</p><p>As more cuts are added:</p><p class="math-container">\[\lim_{K \rightarrow \infty} f^K = \min\limits_{x \in \mathbb{R}^N} f(x)\]</p><h3 id="Bounds"><a class="docs-heading-anchor" href="#Bounds">Bounds</a><a id="Bounds-1"></a><a class="docs-heading-anchor-permalink" href="#Bounds" title="Permalink"></a></h3><p>By convexity, <span>$f^K \le f(x)$</span> for all <span>$x$</span>. Thus, if <span>$x^*$</span> is a minimizer of <span>$f$</span>, then at any point in time we can construct a lower bound for <span>$f(x^*)$</span> by solving <span>$f^K$</span>.</p><p>Moreover, since any feasible point is an upper bound, we can use the primal solution <span>$x^K$</span> returned by solving <span>$f^K$</span> to evaluate <span>$f(x_K)$</span> to generate an upper bound.</p><p>Therefore, <span>$f^K \le f(x^*) \le f(x_K)$</span>.</p><h3 id="Implementation"><a class="docs-heading-anchor" href="#Implementation">Implementation</a><a id="Implementation-1"></a><a class="docs-heading-anchor-permalink" href="#Implementation" title="Permalink"></a></h3><p>Here is pseudo-code fo the Kelley algorithm:</p><ol><li>Take as input a function <span>$f$</span> and a iteration limit <span>$K_{max}$</span>. Set <span>$K = 0$</span>, and initialize <span>$f^K$</span>. Set <span>$lb = -\infty$</span> and <span>$ub = \infty$</span></li><li>Solve <span>$f^K$</span> to obtain a candidate solution <span>$x_{K+1}$</span>.</li><li>Update <span>$lb = f^K$</span> and <span>$ub = \min\{ub, f(x_{K+1}\}$</span></li><li>Add a cut <span>$\theta \ge f(x_{K+1}) + \frac{df}{dx}\left(x_{K+1}\right)^\top (x - x_{K+1})$</span> to form <span>$f^{K+1}$</span>.</li><li>Increment <span>$K$</span></li><li>If <span>$K = K_{max}$</span>, STOP, otherwise, go to step 2.</li></ol><p>And here&#39;s a complete implementation:</p><pre><code class="language-julia">function kelleys_cutting_plane(
    # The function to be minimized.
    f::Function,
    # The gradient of `f`. By default, we use automatic differentiation to
    # compute the gradient of f so the user doesn&#39;t have to!
    dfdx::Function = x -&gt; ForwardDiff.gradient(f, x);
    # The number of arguments to `f`.
    input_dimension::Int,
    # A lower bound for the function `f` over its domain.
    lower_bound::Float64,
    # The number of iterations to run Kelley&#39;s algorithm for before stopping.
    iteration_limit::Int,
)
    # Step (1):
    K = 0
    model = JuMP.Model(GLPK.Optimizer)
    JuMP.@variable(model, θ &gt;= lower_bound)
    JuMP.@variable(model, x[1:input_dimension])
    JuMP.@objective(model, Min, θ)
    lower_bound, upper_bound = -Inf, Inf
    while true
        # Step (2)
        JuMP.optimize!(model)
        x_k = JuMP.value.(x)
        # Step (3)
        lower_bound = JuMP.objective_value(model)
        upper_bound = min(upper_bound, f(x_k))
        println(&quot;K = $K : $(lower_bound) &lt;= f(x*) &lt;= $(upper_bound)&quot;)
        # Step (4):
        c = JuMP.@constraint(model, θ &gt;= f(x_k) + dfdx(x_k)&#39; * (x .- x_k))
        # Step (5):
        K = K + 1
        # Step (6):
        if K == iteration_limit
            break
        end
    end
    println(&quot;Found solution: x_K = &quot;, JuMP.value.(x))
    return
end</code></pre><pre class="documenter-example-output">kelleys_cutting_plane (generic function with 2 methods)</pre><p>Let&#39;s run our algorithm to see what happens:</p><pre><code class="language-julia">kelleys_cutting_plane(
    input_dimension = 2,
    lower_bound = 0.0,
    iteration_limit = 20,
) do x
    return (x[1] - 1)^2 + (x[2] + 2)^2
end</code></pre><pre class="documenter-example-output">K = 0 : 0.0 &lt;= f(x*) &lt;= 5.0
K = 1 : 0.0 &lt;= f(x*) &lt;= 1.5625
K = 2 : 0.0 &lt;= f(x*) &lt;= 1.5625
K = 3 : 0.0 &lt;= f(x*) &lt;= 0.6103515625000003
K = 4 : 0.0 &lt;= f(x*) &lt;= 0.42385525173611077
K = 5 : 0.0 &lt;= f(x*) &lt;= 0.16556845770941836
K = 6 : 0.0 &lt;= f(x*) &lt;= 0.11497809563154059
K = 7 : 0.0 &lt;= f(x*) &lt;= 0.04491331860607062
K = 8 : 0.0 &lt;= f(x*) &lt;= 0.031189804587549008
K = 9 : 0.0 &lt;= f(x*) &lt;= 0.012183517417011335
K = 10 : 0.0 &lt;= f(x*) &lt;= 0.008460775984035652
K = 11 : 0.0 &lt;= f(x*) &lt;= 0.003304990618763928
K = 12 : 0.0 &lt;= f(x*) &lt;= 0.0022951323741416145
K = 13 : 0.0 &lt;= f(x*) &lt;= 0.0008965360836490337
K = 14 : 0.0 &lt;= f(x*) &lt;= 0.0006225945025340471
K = 15 : 0.0 &lt;= f(x*) &lt;= 0.00024320097755236005
K = 16 : 0.0 &lt;= f(x*) &lt;= 0.00016888956774469338
K = 17 : 0.0 &lt;= f(x*) &lt;= 6.597248740026785e-5
K = 18 : 0.0 &lt;= f(x*) &lt;= 4.581422736130041e-5
K = 19 : 0.0 &lt;= f(x*) &lt;= 1.7896182563008953e-5
Found solution: x_K = [0.997655, -2.00352]</pre><h2 id="Preliminaries:-approximating-the-cost-to-go-term"><a class="docs-heading-anchor" href="#Preliminaries:-approximating-the-cost-to-go-term">Preliminaries: approximating the cost-to-go term</a><a id="Preliminaries:-approximating-the-cost-to-go-term-1"></a><a class="docs-heading-anchor-permalink" href="#Preliminaries:-approximating-the-cost-to-go-term" title="Permalink"></a></h2><p>In the background theory section, we discussed how you could formulate an optimal policy to a multistage stochastic program using the dynamic programming recursion:</p><p class="math-container">\[\begin{aligned}
V_i(x, \omega) = \min\limits_{\bar{x}, x^\prime, u} \;\; &amp; C_i(\bar{x}, u, \omega) + \mathbb{E}_{j \in i^+, \varphi \in \Omega_j}[V_j(x^\prime, \varphi)]\\
&amp; x^\prime = T_i(\bar{x}, u, \omega) \\
&amp; u \in U_i(\bar{x}, \omega) \\
&amp; \bar{x} = x
\end{aligned}\]</p><p>where our decision rule, <span>$\pi_i(x, \omega)$</span>, solves this optimization problem and returns a <span>$u^*$</span> corresponding to an optimal solution. Moreover, we alluded to the fact that the cost-to-go term (the nasty recursive expectation) makes this problem intractable to solve.</p><p>However, if, excluding the cost-to-go term, <span>$V_i(x, \omega)$</span> can be formulated as a linear program (this also works for convex programs, but the math is more involved), then we can make some progress.</p><p>First, notice that <span>$x$</span> only appears as a right-hand side term of <span>$V_i$</span>. Therefore, <span>$V_i(x, \cdot)$</span> is convex with respect to <span>$x$</span> for fixed <span>$\omega$</span>. Moreover, the reduced cost of the decision variable <span>$\bar{x}$</span> is a subgradient of the function <span>$V_i$</span> with respect to <span>$x$</span>! (This is one reason why we add the <span>$\bar{x}$</span> and the fishing constraint <span>$\bar{x} = x$</span>.)</p><p>Second, a convex combination of convex functions is also convex, so the cost-to-go term is a convex function of <span>$x^\prime$</span>.</p><p>Stochastic dual dynamic programming converts this problem into a tractable form by applying Kelley&#39;s cutting plane algorithm to the cost-to-go term:</p><p class="math-container">\[\begin{aligned}
V_i^K(x, \omega) = \min\limits_{\bar{x}, x^\prime, u} \;\; &amp; C_i(\bar{x}, u, \omega) + \theta\\
&amp; x^\prime = T_i(\bar{x}, u, \omega) \\
&amp; u \in U_i(\bar{x}, \omega) \\
&amp; \bar{x} = x \\
&amp; \theta \ge \mathbb{E}_{j \in i^+, \varphi \in \Omega_j}\left[V_j^k(x^\prime_k, \varphi) + \frac{dV_j^k}{dx^\prime}\left(x^\prime_k, \varphi\right)^\top (x^\prime - x^\prime_k)\right],\quad k=1,\ldots,K \\
&amp; \theta \ge M
\end{aligned}\]</p><p>All we need now is a way of generating these cutting planes in an iterative manner. Before we get to that though, let&#39;s start writing some code.</p><h2 id="Implementation:-modeling"><a class="docs-heading-anchor" href="#Implementation:-modeling">Implementation: modeling</a><a id="Implementation:-modeling-1"></a><a class="docs-heading-anchor-permalink" href="#Implementation:-modeling" title="Permalink"></a></h2><p>Let&#39;s make a start by defining the problem structure. Like SDDP.jl, we need a few things:</p><ol><li>A description of the structure of the policy graph: how many nodes there are, and the arcs linking the nodes together with their corresponding probabilities.</li><li>A JuMP model for each node in the policy graph</li><li>A way to identify the incoming and outgoing state variables of each node</li><li>A description of the random variable, as well as a function that we can call that will modify the JuMP model to reflect the realization of the random variable.</li><li>A decision variable to act as the approximated cost-to-go term.</li></ol><h3 id="Structs"><a class="docs-heading-anchor" href="#Structs">Structs</a><a id="Structs-1"></a><a class="docs-heading-anchor-permalink" href="#Structs" title="Permalink"></a></h3><p>The first struct we are going to use is a <code>State</code> struct that will wrap an incoming and outgoing state variable.</p><pre><code class="language-julia">struct State
    in::JuMP.VariableRef
    out::JuMP.VariableRef
end</code></pre><p>Next, we need a struct to wrap all of the uncertainty within a node.</p><pre><code class="language-julia">struct Uncertainty
    parameterize::Function
    Ω::Vector{Any}
    P::Vector{Float64}
end</code></pre><p><code>parameterize</code> is a function, which takes a realization of the random variable <span>$\omega\in\Omega$</span> and updates the subproblem accordingly. The finite discrete random variable is defined by the vectors <code>Ω</code> and <code>P</code>, so that the random variable takes the value <code>Ω[i]</code> with probability <code>P[i]</code>. As such, <code>P</code> should sum to 1. (We don&#39;t check this here, but we should; we do in SDDP.jl.)</p><p>Now we have two building blocks, we can declare the structure of each node.</p><pre><code class="language-julia">struct Node
    subproblem::JuMP.Model
    states::Dict{Symbol,State}
    uncertainty::Uncertainty
    cost_to_go::JuMP.VariableRef
end</code></pre><ul><li><code>subproblem</code> is going to be the JuMP model that we build at each node.</li><li><code>states</code> is a dictionary that maps a symbolic name of a state variable to a <code>State</code> object wrapping the incoming and outgoing state variables in <code>subproblem</code>.</li><li><code>uncertainty</code> is an <code>Uncertainty</code> object described above.</li><li><code>cost_to_go</code> is a JuMP variable that approximates the cost-to-go term.</li></ul><p>Finally, we define a simplified policy graph as follows:</p><pre><code class="language-julia">struct PolicyGraph
    nodes::Vector{Node}
    arcs::Vector{Dict{Int,Float64}}
end</code></pre><p>There is a vector of nodes, as well as a data structure for the arcs. <code>arcs</code> is a vector of dictionaries, where <code>arcs[i][j]</code> gives the probabiltiy of transitioning from node <code>i</code> to node <code>j</code>, if an arc exists.</p><p>To simplify things, we will assume that the root node transitions to node <code>1</code> with probability 1, and there are no other incoming arcs to node 1. Notably, we can still define cyclic graphs though!</p><p>We also define a nice <code>show</code> method so that we don&#39;t accidentally print a large amount of information to the screen when creating a model.</p><pre><code class="language-julia">function Base.show(io::IO, model::PolicyGraph)
    println(io, &quot;A policy graph with $(length(model.nodes)) nodes&quot;)
    println(io, &quot;Arcs:&quot;)
    for (from, arcs) in enumerate(model.arcs)
        for (to, probability) in arcs
            println(io, &quot;  $(from) =&gt; $(to) w.p. $(probability)&quot;)
        end
    end
    return
end</code></pre><h3 id="Functions"><a class="docs-heading-anchor" href="#Functions">Functions</a><a id="Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Functions" title="Permalink"></a></h3><p>Now we have some basic types, let&#39;s implement some functions so that the user can create a model.</p><p>First, we need an example of a function that the user will provide. Like SDDP.jl, this takes an empty <code>subproblem</code>, and a node index, in this case <code>t::Int</code>. You could change this function to change the model, or define a new one later in the code.</p><pre><code class="language-julia">function subproblem_builder(subproblem::JuMP.Model, t::Int)
    # Define the state variables. Note how we fix the incoming state to the
    # initial state variable regardless of `t`! This isn&#39;t strictly necessary;
    # it only matters that we do it for the first node.
    JuMP.@variable(subproblem, volume_in == 200)
    JuMP.@variable(subproblem, 0 &lt;= volume_out &lt;= 200)
    states = Dict(:volume =&gt; State(volume_in, volume_out))
    # Define the control variables.
    JuMP.@variables(subproblem, begin
        thermal_generation &gt;= 0
        hydro_generation   &gt;= 0
        hydro_spill        &gt;= 0
        inflow
    end)
    # Define the constraints
    JuMP.@constraints(subproblem, begin
        volume_out == volume_in + inflow - hydro_generation - hydro_spill
        demand_constraint, thermal_generation + hydro_generation == 150.0
    end)
    # Define the objective for each stage `t`. Note that we can use `t` as an
    # index for t = 1, 2, 3.
    fuel_cost = [50.0, 100.0, 150.0]
    JuMP.@objective(subproblem, Min, fuel_cost[t] * thermal_generation)
    # Finally, we define the uncertainty object. Because this is a simplified
    # implementation of SDDP, we shall politely ask the user to only modify the
    # constraints, and not the objective function! (Not that it changes the
    # algorithm, we just have to add more information to keep track of things.)
    uncertainty = Uncertainty([0.0, 50.0, 100.0], [1 / 3, 1 / 3, 1 / 3]) do ω
        JuMP.fix(inflow, ω)
    end
    return states, uncertainty
end</code></pre><pre class="documenter-example-output">subproblem_builder (generic function with 1 method)</pre><p>If you&#39;ve read <a href="../02_adding_uncertainty/#Basic-II:-adding-uncertainty">Basic II: adding uncertainty</a>, this example should be familiar. You can probably see how some of the SDDP.jl functionality like <a href="../../apireference/#SDDP.@stageobjective"><code>@stageobjective</code></a> and <a href="../../apireference/#SDDP.parameterize"><code>SDDP.parameterize</code></a> help smooth some of the usability issues like needing to construct both the incoming and outgoing state variables, or needing to explicitly <code>return states, uncertainty</code>.</p><p>The next function we need to define is the analog of <a href="../../apireference/#SDDP.PolicyGraph"><code>SDDP.PolicyGraph</code></a>. It should be pretty readable.</p><pre><code class="language-julia">function PolicyGraph(
    subproblem_builder::Function;
    graph::Vector{Dict{Int,Float64}},
    lower_bound::Float64,
    optimizer,
)
    nodes = Node[]
    for t = 1:length(graph)
        # Create a model.
        model = JuMP.Model(optimizer)
        # Use the provided function to build out each subproblem. The user&#39;s
        # function returns a dictionary mapping `Symbol`s to `State` objects,
        # and an `Uncertainty` object.
        states, uncertainty = subproblem_builder(model, t)
        # Now add the cost-to-go terms:
        JuMP.@variable(model, cost_to_go &gt;= lower_bound)
        obj = JuMP.objective_function(model)
        JuMP.@objective(model, Min, obj + cost_to_go)
        # If there are no outgoing arcs, the cost-to-go is 0.0.
        if length(graph[t]) == 0
            JuMP.fix(cost_to_go, 0.0; force = true)
        end
        push!(nodes, Node(model, states, uncertainty, cost_to_go))
    end
    return PolicyGraph(nodes, graph)
end</code></pre><pre class="documenter-example-output">Main.ex-21_pedagogical_sddp.PolicyGraph</pre><p>Then, we can create a model using the <code>subproblem_builder</code> function we defined earlier:</p><pre><code class="language-julia">model = PolicyGraph(
    subproblem_builder;
    graph = [
        Dict(2 =&gt; 1.0),
        Dict(3 =&gt; 1.0),
        Dict{Int,Float64}(),
    ],
    lower_bound = 0.0,
    optimizer = GLPK.Optimizer,
)</code></pre><pre class="documenter-example-output">A policy graph with 3 nodes
Arcs:
  1 =&gt; 2 w.p. 1.0
  2 =&gt; 3 w.p. 1.0
</pre><h2 id="Implementation:-solution-algorithm"><a class="docs-heading-anchor" href="#Implementation:-solution-algorithm">Implementation: solution algorithm</a><a id="Implementation:-solution-algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#Implementation:-solution-algorithm" title="Permalink"></a></h2><p>Before we get properly coding the solution algorithm, it&#39;s also going to be useful to have a function that samples a realization of the random variable defined by <code>Ω</code> and <code>P</code>:</p><pre><code class="language-julia">function sample_uncertainty(uncertainty::Uncertainty)
    r = rand()
    for (p, ω) in zip(uncertainty.P, uncertainty.Ω)
        r -= p
        if r &lt; 0.0
            return ω
        end
    end
    error(&quot;We should never get here because P should sum to 1.0.&quot;)
end</code></pre><pre class="documenter-example-output">sample_uncertainty (generic function with 1 method)</pre><p>You should be able to work out what is going on. <code>rand()</code> samples a uniform random variable in <code>[0, 1)</code>. For example:</p><pre><code class="language-julia">for _ = 1:3
    println(&quot;ω = &quot;, sample_uncertainty(model.nodes[1].uncertainty))
end</code></pre><pre class="documenter-example-output">ω = 0.0
ω = 0.0
ω = 100.0</pre><p>It&#39;s also going to be useful to define a function that generates a random walk through the nodes of the graph:</p><pre><code class="language-julia">function sample_next_node(model::PolicyGraph, current::Int)
    r = rand()
    for (to, probability) in model.arcs[current]
        r -= probability
        if r &lt; 0.0
            return to
        end
    end
    # We looped through the outgoing arcs and still have probability left over!
    # This means we&#39;ve hit a leaf node and it&#39;s time to stop walking.
    return nothing
end</code></pre><pre class="documenter-example-output">sample_next_node (generic function with 1 method)</pre><p>For example:</p><pre><code class="language-julia">for i = 1:3
    # We use `repr` to print the next node, because `sample_next_node` can
    # return `nothing`.
    println(&quot;Next node from $(i) = &quot;, repr(sample_next_node(model, i)))
end</code></pre><pre class="documenter-example-output">Next node from 1 = 2
Next node from 2 = 3
Next node from 3 = nothing</pre><p>This is a little boring, because our graph is simple. However, more complicated graphs will generate more interesting trajectories!</p><h2 id="Implementation:-the-forward-pass"><a class="docs-heading-anchor" href="#Implementation:-the-forward-pass">Implementation: the forward pass</a><a id="Implementation:-the-forward-pass-1"></a><a class="docs-heading-anchor-permalink" href="#Implementation:-the-forward-pass" title="Permalink"></a></h2><p>Like Kelley&#39;s algorithm, we need a way of generating candidate solutions <span>$x_K$</span>. However, unlike the Kelley&#39;s example, our functions need two inputs: an incoming state variable and a realization of the random variable. We get these from a simulation of the policy, which we call the <strong>forward pass</strong>.</p><p>The forward pass walks the policy graph from start to end, transitioning randomly along the arcs. At each node, it observes a realization of the random variable and solves the approximated subproblem to generate a candidate outgoing state variable <span>$x_k^\prime$</span>. The outgoing state variable is passed as the incoming state variable to the next node in the trajectory.</p><pre><code class="language-julia">function forward_pass(
    model::PolicyGraph,
    io::IO = stdout,
)
    println(io, &quot;| Forward Pass&quot;)
    # First, get the value of the state at the root node (e.g., x_R).
    incoming_state = Dict(
        k =&gt; JuMP.fix_value(v.in) for (k, v) in model.nodes[1].states
    )
    # `simulation_cost` is an accumlator that is going to sum the stage-costs
    # incurred over the forward pass.
    simulation_cost = 0.0
    # We also need to record the nodes visited and resultant outgoing state
    # variables so we can pass them to the backward pass.
    trajectory = Tuple{Int,Dict{Symbol,Float64}}[]
    # Now&#39;s the meat of the forward pass: beginning at the first node:
    t = 1
    while t !== nothing
        node = model.nodes[t]
        println(io, &quot;| | Visiting node $(t)&quot;)
        # Sample the uncertainty:
        ω = sample_uncertainty(node.uncertainty)
        println(io, &quot;| | | ω = &quot;, ω)
        # Parameterizing the subproblem using the user-provided function:
        node.uncertainty.parameterize(ω)
        println(io, &quot;| | | x = &quot;, incoming_state)
        # Update the incoming state variable:
        for (k, v) in incoming_state
            JuMP.fix(node.states[k].in, v; force = true)
        end
        # Now solve the subproblem and check we found an optimal solution:
        JuMP.optimize!(node.subproblem)
        if JuMP.termination_status(node.subproblem) != JuMP.MOI.OPTIMAL
            error(&quot;Something went terribly wrong!&quot;)
        end
        # Compute the outgoing state variables:
        outgoing_state = Dict(k =&gt; JuMP.value(v.out) for (k, v) in node.states)
        println(io, &quot;| | | x′ = &quot;, outgoing_state)
        # We also need to compute the stage cost to add to our
        # `simulation_cost` accumulator:
        stage_cost = JuMP.objective_value(node.subproblem) - JuMP.value(node.cost_to_go)
        simulation_cost += stage_cost
        println(io, &quot;| | | C(x, u, ω) = &quot;, stage_cost)
        # As a penultimate step, set the outgoing state of stage t and the
        # incoming state of stage t + 1, and add the node to the trajectory.
        incoming_state = outgoing_state
        push!(trajectory, (t, outgoing_state))
        # Finally, sample a new node to step to. If `t === nothing`, the
        # `while` loop will break.
        t = sample_next_node(model, t)
    end
    return trajectory, simulation_cost
end</code></pre><pre class="documenter-example-output">forward_pass (generic function with 2 methods)</pre><p>Let&#39;s take a look at one forward pass:</p><pre><code class="language-julia">trajectory, simulation_cost = forward_pass(model);</code></pre><pre class="documenter-example-output">| Forward Pass
| | Visiting node 1
| | | ω = 50.0
| | | x = Dict(:volume=&gt;200.0)
| | | x′ = Dict(:volume=&gt;100.0)
| | | C(x, u, ω) = 0.0
| | Visiting node 2
| | | ω = 0.0
| | | x = Dict(:volume=&gt;100.0)
| | | x′ = Dict(:volume=&gt;0.0)
| | | C(x, u, ω) = 5000.0
| | Visiting node 3
| | | ω = 0.0
| | | x = Dict(:volume=&gt;0.0)
| | | x′ = Dict(:volume=&gt;0.0)
| | | C(x, u, ω) = 22500.0</pre><h2 id="Implementation:-the-backward-pass"><a class="docs-heading-anchor" href="#Implementation:-the-backward-pass">Implementation: the backward pass</a><a id="Implementation:-the-backward-pass-1"></a><a class="docs-heading-anchor-permalink" href="#Implementation:-the-backward-pass" title="Permalink"></a></h2><p>From the forward pass, we obtained a vector of nodes visted and their corresponding outgoing state variables. In the <strong>backward pass</strong>, we walk back up this list, and at each node, we compute the cut:</p><p class="math-container">\[\theta \ge \mathbb{E}_{j \in i^+, \varphi \in \Omega_j}\left[V_j^k(x^\prime_k, \varphi) + \frac{dV_j^k}{dx^\prime}\left(x^\prime_k, \varphi\right)^\top (x^\prime - x^\prime_k)\right],\quad k=1,\ldots,K\]</p><pre><code class="language-julia">function backward_pass(
    model::PolicyGraph,
    trajectory::Vector{Tuple{Int,Dict{Symbol,Float64}}},
    io::IO = stdout,
)
    println(io, &quot;| Backward pass&quot;)
    # For the backward pass, we walk back up the nodes.
    for i = reverse(1:length(trajectory))
        index, outgoing_states = trajectory[i]
        node = model.nodes[index]
        println(io, &quot;| | Visiting node $(index)&quot;)
        if length(model.arcs[index]) == 0
            # If there are no children, the cost-to-go is 0.
            println(io, &quot;| | | Skipping node because the cost-to-go is 0&quot;)
            continue
        end
        # Create an empty affine expression that we will use to build up the
        # right-hand side of the cut expression.
        cut_expression = JuMP.AffExpr(0.0)
        # Now for each possible node and realization of the uncertainty, solve
        # the subproblem, and add `P_ij * p_ω * [V + dVdxᵀ(x - x_k)]` to the
        # cut expression.
        for (j, P_ij) in model.arcs[index]
            next_node = model.nodes[j]
            for (k, v) in outgoing_states
                JuMP.fix(next_node.states[k].in, v; force = true)
            end
            for (pω, ω) in zip(next_node.uncertainty.P, next_node.uncertainty.Ω)
                println(io, &quot;| | | Solving ω = &quot;, ω)
                next_node.uncertainty.parameterize(ω)
                JuMP.optimize!(next_node.subproblem)
                V = JuMP.objective_value(next_node.subproblem)
                println(io, &quot;| | | | V = &quot;, V)
                dVdx = Dict(k =&gt; JuMP.reduced_cost(v.in) for (k, v) in next_node.states)
                println(io, &quot;| | | | dVdx′ = &quot;, dVdx)
                cut_expression += P_ij * pω * JuMP.@expression(
                    node.subproblem,
                    V + sum(
                        dVdx[k] * (x.out - outgoing_states[k])
                        for (k, x) in node.states
                    ),
                )
            end
        end
        # And then refine the cost-to-go variable by adding a cut that is the
        # expectation of the cuts computed in the step above.
        c = JuMP.@constraint(
            node.subproblem, node.cost_to_go &gt;= cut_expression
        )
        println(io, &quot;| | | Adding cut : &quot;, c)
    end
    return nothing
end</code></pre><pre class="documenter-example-output">backward_pass (generic function with 2 methods)</pre><h2 id="Implementation:-the-training-loop"><a class="docs-heading-anchor" href="#Implementation:-the-training-loop">Implementation: the training loop</a><a id="Implementation:-the-training-loop-1"></a><a class="docs-heading-anchor-permalink" href="#Implementation:-the-training-loop" title="Permalink"></a></h2><h3 id="Lower-bounds"><a class="docs-heading-anchor" href="#Lower-bounds">Lower bounds</a><a id="Lower-bounds-1"></a><a class="docs-heading-anchor-permalink" href="#Lower-bounds" title="Permalink"></a></h3><p>Recall from Kelley&#39;s that we can obtain a lower bound for <span>$f(x^*)$</span> be evaluating <span>$f^K$</span>. The analagous lower bound for a multistage stochastic program is:</p><p class="math-container">\[\mathbb{E}_{i \in R^+, \omega \in \Omega_i}[V_i^K(x_R, \omega)] \le \min_{\pi} \mathbb{E}_{i \in R^+, \omega \in \Omega_i}[V_i^\pi(x_R, \omega)]\]</p><p>Here&#39;s how we compute the lower bound:</p><pre><code class="language-julia">function lower_bound(model::PolicyGraph)
    node = model.nodes[1]
    bound = 0.0
    for (p, ω) in zip(node.uncertainty.P, node.uncertainty.Ω)
        node.uncertainty.parameterize(ω)
        JuMP.optimize!(node.subproblem)
        bound += p * JuMP.objective_value(node.subproblem)
    end
    return bound
end</code></pre><pre class="documenter-example-output">lower_bound (generic function with 1 method)</pre><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>The implementation is simplified because we assumed that there is only one arc from the root node, and that it pointed to the first node in the</p></div></div><p>vector.</p><p>Because we haven&#39;t trained a policy yet, the lower bound is going to be very bad:</p><pre><code class="language-julia">lower_bound(model)</code></pre><pre class="documenter-example-output">0.0</pre><h3 id="Upper-bounds"><a class="docs-heading-anchor" href="#Upper-bounds">Upper bounds</a><a id="Upper-bounds-1"></a><a class="docs-heading-anchor-permalink" href="#Upper-bounds" title="Permalink"></a></h3><p>With Kelley&#39;s algorithm, we could easily construct an upper bound by evaluating <span>$f(x_K)$</span>. However, it is almost always intractable to evaluate an upper bound for multistage stochastic programs due to the large number of nodes and the nested expectations. Instead, we can perform a Monte Carlo simulation of the policy to build a statistical estimate for the value of <span>$\mathbb{E}_{i \in R^+, \omega \in \Omega_i}[V_i^\pi(x_R, \omega)]$</span>, where <span>$\pi$</span> is the policy defined by the current approximations <span>$V^K_i$</span>.</p><pre><code class="language-julia">function upper_bound(model::PolicyGraph; replications::Int)
    # Pipe the output to `devnull` so we don&#39;t print too much!
    simulations = [forward_pass(model, devnull) for _ = 1:replications]
    z = [s[2] for s in simulations]
    μ  = Statistics.mean(z)
    tσ = 1.96 * Statistics.std(z) / sqrt(replications)
    return μ, tσ
end</code></pre><pre class="documenter-example-output">upper_bound (generic function with 1 method)</pre><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>The width of the confidence interval is incorrect if there are cycles in the graph, because the distribution of simulation costs <code>z</code> is not symmetric. The mean is correct, however.</p></div></div><h3 id="Training"><a class="docs-heading-anchor" href="#Training">Training</a><a id="Training-1"></a><a class="docs-heading-anchor-permalink" href="#Training" title="Permalink"></a></h3><p>The <code>train</code> loop of SDDP just applies the forward and backward passes iteratively, followed by a final simulation to compute the upper bound confidence interval:</p><pre><code class="language-julia">function train(
    model::PolicyGraph;
    iteration_limit::Int,
    replications::Int,
    io::IO = stdout,
)
    for i = 1:iteration_limit
        println(io, &quot;Starting iteration $(i)&quot;)
        outgoing_states, _ = forward_pass(model, io)
        backward_pass(model, outgoing_states, io)
        println(io, &quot;| Finished iteration&quot;)
        println(io, &quot;| | lower_bound = &quot;, lower_bound(model))
    end
    μ, tσ = upper_bound(model; replications = replications)
    println(io, &quot;Upper bound = $(μ) ± $(tσ)&quot;)
    return
end</code></pre><pre class="documenter-example-output">train (generic function with 1 method)</pre><p>Using our <code>model</code> we defined earlier, we can go:</p><pre><code class="language-julia">train(model; iteration_limit = 3, replications = 100)</code></pre><pre class="documenter-example-output">Starting iteration 1
| Forward Pass
| | Visiting node 1
| | | ω = 50.0
| | | x = Dict(:volume=&gt;200.0)
| | | x′ = Dict(:volume=&gt;100.0)
| | | C(x, u, ω) = 0.0
| | Visiting node 2
| | | ω = 50.0
| | | x = Dict(:volume=&gt;100.0)
| | | x′ = Dict(:volume=&gt;0.0)
| | | C(x, u, ω) = 0.0
| | Visiting node 3
| | | ω = 100.0
| | | x = Dict(:volume=&gt;0.0)
| | | x′ = Dict(:volume=&gt;0.0)
| | | C(x, u, ω) = 7500.0
| Backward pass
| | Visiting node 3
| | | Skipping node because the cost-to-go is 0
| | Visiting node 2
| | | Solving ω = 0.0
| | | | V = 22500.0
| | | | dVdx′ = Dict(:volume=&gt;-150.0)
| | | Solving ω = 50.0
| | | | V = 15000.0
| | | | dVdx′ = Dict(:volume=&gt;-150.0)
| | | Solving ω = 100.0
| | | | V = 7500.0
| | | | dVdx′ = Dict(:volume=&gt;-150.0)
| | | Adding cut : 150 volume_out + cost_to_go ≥ 15000.0
| | Visiting node 1
| | | Solving ω = 0.0
| | | | V = 15000.0
| | | | dVdx′ = Dict(:volume=&gt;-100.0)
| | | Solving ω = 50.0
| | | | V = 10000.0
| | | | dVdx′ = Dict(:volume=&gt;-100.0)
| | | Solving ω = 100.0
| | | | V = 5000.0
| | | | dVdx′ = Dict(:volume=&gt;-100.0)
| | | Adding cut : 99.99999999999999 volume_out + cost_to_go ≥ 20000.0
| Finished iteration
| | lower_bound = 5000.000000000002
Starting iteration 2
| Forward Pass
| | Visiting node 1
| | | ω = 0.0
| | | x = Dict(:volume=&gt;200.0)
| | | x′ = Dict(:volume=&gt;200.0)
| | | C(x, u, ω) = 7500.000000000002
| | Visiting node 2
| | | ω = 100.0
| | | x = Dict(:volume=&gt;200.0)
| | | x′ = Dict(:volume=&gt;100.0)
| | | C(x, u, ω) = 0.0
| | Visiting node 3
| | | ω = 100.0
| | | x = Dict(:volume=&gt;100.0)
| | | x′ = Dict(:volume=&gt;50.0)
| | | C(x, u, ω) = 0.0
| Backward pass
| | Visiting node 3
| | | Skipping node because the cost-to-go is 0
| | Visiting node 2
| | | Solving ω = 0.0
| | | | V = 7500.0
| | | | dVdx′ = Dict(:volume=&gt;-150.0)
| | | Solving ω = 50.0
| | | | V = 0.0
| | | | dVdx′ = Dict(:volume=&gt;-150.0)
| | | Solving ω = 100.0
| | | | V = 0.0
| | | | dVdx′ = Dict(:volume=&gt;0.0)
| | | Adding cut : 100 volume_out + cost_to_go ≥ 12500.0
| | Visiting node 1
| | | Solving ω = 0.0
| | | | V = 7499.999999999997
| | | | dVdx′ = Dict(:volume=&gt;-100.0)
| | | Solving ω = 50.0
| | | | V = 2499.9999999999973
| | | | dVdx′ = Dict(:volume=&gt;-100.0)
| | | Solving ω = 100.0
| | | | V = 0.0
| | | | dVdx′ = Dict(:volume=&gt;0.0)
| | | Adding cut : 66.66666666666666 volume_out + cost_to_go ≥ 16666.666666666664
| Finished iteration
| | lower_bound = 8333.333333333332
Starting iteration 3
| Forward Pass
| | Visiting node 1
| | | ω = 100.0
| | | x = Dict(:volume=&gt;200.0)
| | | x′ = Dict(:volume=&gt;200.0)
| | | C(x, u, ω) = 2500.0
| | Visiting node 2
| | | ω = 0.0
| | | x = Dict(:volume=&gt;200.0)
| | | x′ = Dict(:volume=&gt;125.0)
| | | C(x, u, ω) = 7500.0
| | Visiting node 3
| | | ω = 100.0
| | | x = Dict(:volume=&gt;125.0)
| | | x′ = Dict(:volume=&gt;75.0)
| | | C(x, u, ω) = 0.0
| Backward pass
| | Visiting node 3
| | | Skipping node because the cost-to-go is 0
| | Visiting node 2
| | | Solving ω = 0.0
| | | | V = 3750.0
| | | | dVdx′ = Dict(:volume=&gt;-150.0)
| | | Solving ω = 50.0
| | | | V = 0.0
| | | | dVdx′ = Dict(:volume=&gt;0.0)
| | | Solving ω = 100.0
| | | | V = 0.0
| | | | dVdx′ = Dict(:volume=&gt;0.0)
| | | Adding cut : 50 volume_out + cost_to_go ≥ 7500.0
| | Visiting node 1
| | | Solving ω = 0.0
| | | | V = 7500.0
| | | | dVdx′ = Dict(:volume=&gt;-100.0)
| | | Solving ω = 50.0
| | | | V = 2500.0
| | | | dVdx′ = Dict(:volume=&gt;-100.0)
| | | Solving ω = 100.0
| | | | V = 0.0
| | | | dVdx′ = Dict(:volume=&gt;0.0)
| | | Adding cut : 66.66666666666666 volume_out + cost_to_go ≥ 16666.666666666664
| Finished iteration
| | lower_bound = 8333.333333333332
Upper bound = 8950.0 ± 995.1764271786793</pre><p>Success! We trained a policy for a finite horizon multistage stochastic program using stochastic dual dynamic programming.</p><h2 id="Implementation:-decision-rules"><a class="docs-heading-anchor" href="#Implementation:-decision-rules">Implementation: decision rules</a><a id="Implementation:-decision-rules-1"></a><a class="docs-heading-anchor-permalink" href="#Implementation:-decision-rules" title="Permalink"></a></h2><p>A final step is the ability to evaluate the decision rule associated with a node:</p><pre><code class="language-julia">function decision_rule(
    node::Node;
    incoming_state::Dict{Symbol,Float64},
    random_variable,
)
    node.uncertainty.parameterize(random_variable)
    for (k, v) in incoming_state
        JuMP.fix(node.states[k].in, v; force = true)
    end
    JuMP.optimize!(node.subproblem)
    return Dict(
        k =&gt; JuMP.value.(v)
        for (k, v) in JuMP.object_dictionary(node.subproblem)
    )
end

decision_rule(
    model.nodes[1];
    incoming_state = Dict(:volume =&gt; 150.0),
    random_variable = 75,
)</code></pre><pre class="documenter-example-output">Dict{Symbol,Float64} with 8 entries:
  :volume_out         =&gt; 200.0
  :demand_constraint  =&gt; 150.0
  :hydro_spill        =&gt; 0.0
  :inflow             =&gt; 75.0
  :volume_in          =&gt; 150.0
  :thermal_generation =&gt; 125.0
  :hydro_generation   =&gt; 25.0
  :cost_to_go         =&gt; 3333.33</pre><p>Note how the random variable can be <strong>out-of-sample</strong>, i.e., it doesn&#39;t have to be in the vector <span>$\Omega$</span> we created when defining the model! This is a notable difference to other multistage stochastic solution methods like progressive hedging or using the deterministic equivalent.</p><h2 id="Example:-infinite-horizon"><a class="docs-heading-anchor" href="#Example:-infinite-horizon">Example: infinite horizon</a><a id="Example:-infinite-horizon-1"></a><a class="docs-heading-anchor-permalink" href="#Example:-infinite-horizon" title="Permalink"></a></h2><p>As promised earlier, our implementation is actually pretty general. It can solve any multistage stochastic (linear) program defined by a policy graph, including infinite horizon problems!</p><p>Here&#39;s an example, where we have extended our earlier problem with an arc from node 3 to node 2 with probability 0.5. You can interpret the 0.5 as a discount factor.</p><pre><code class="language-julia">model = PolicyGraph(
    subproblem_builder;
    graph = [
        Dict(2 =&gt; 1.0),
        Dict(3 =&gt; 1.0),
        Dict(2 =&gt; 0.5),
    ],
    lower_bound = 0.0,
    optimizer = GLPK.Optimizer,
)</code></pre><pre class="documenter-example-output">A policy graph with 3 nodes
Arcs:
  1 =&gt; 2 w.p. 1.0
  2 =&gt; 3 w.p. 1.0
  3 =&gt; 2 w.p. 0.5
</pre><p>Then, train a policy:</p><pre><code class="language-julia">train(model; iteration_limit = 3, replications = 100)</code></pre><pre class="documenter-example-output">Starting iteration 1
| Forward Pass
| | Visiting node 1
| | | ω = 50.0
| | | x = Dict(:volume=&gt;200.0)
| | | x′ = Dict(:volume=&gt;100.0)
| | | C(x, u, ω) = 0.0
| | Visiting node 2
| | | ω = 100.0
| | | x = Dict(:volume=&gt;100.0)
| | | x′ = Dict(:volume=&gt;50.0)
| | | C(x, u, ω) = 0.0
| | Visiting node 3
| | | ω = 0.0
| | | x = Dict(:volume=&gt;50.0)
| | | x′ = Dict(:volume=&gt;0.0)
| | | C(x, u, ω) = 15000.0
| | Visiting node 2
| | | ω = 0.0
| | | x = Dict(:volume=&gt;0.0)
| | | x′ = Dict(:volume=&gt;0.0)
| | | C(x, u, ω) = 15000.0
| | Visiting node 3
| | | ω = 100.0
| | | x = Dict(:volume=&gt;0.0)
| | | x′ = Dict(:volume=&gt;0.0)
| | | C(x, u, ω) = 7500.0
| | Visiting node 2
| | | ω = 100.0
| | | x = Dict(:volume=&gt;0.0)
| | | x′ = Dict(:volume=&gt;0.0)
| | | C(x, u, ω) = 5000.0
| | Visiting node 3
| | | ω = 100.0
| | | x = Dict(:volume=&gt;0.0)
| | | x′ = Dict(:volume=&gt;0.0)
| | | C(x, u, ω) = 7500.0
| Backward pass
| | Visiting node 3
| | | Solving ω = 0.0
| | | | V = 15000.0
| | | | dVdx′ = Dict(:volume=&gt;-100.0)
| | | Solving ω = 50.0
| | | | V = 10000.0
| | | | dVdx′ = Dict(:volume=&gt;-100.0)
| | | Solving ω = 100.0
| | | | V = 5000.0
| | | | dVdx′ = Dict(:volume=&gt;-100.0)
| | | Adding cut : 49.99999999999999 volume_out + cost_to_go ≥ 4999.999999999999
| | Visiting node 2
| | | Solving ω = 0.0
| | | | V = 27500.0
| | | | dVdx′ = Dict(:volume=&gt;-150.0)
| | | Solving ω = 50.0
| | | | V = 20000.0
| | | | dVdx′ = Dict(:volume=&gt;-150.0)
| | | Solving ω = 100.0
| | | | V = 12500.0
| | | | dVdx′ = Dict(:volume=&gt;-150.0)
| | | Adding cut : 150 volume_out + cost_to_go ≥ 20000.0
| | Visiting node 3
| | | Solving ω = 0.0
| | | | V = 35000.0
| | | | dVdx′ = Dict(:volume=&gt;-150.0)
| | | Solving ω = 50.0
| | | | V = 27500.0
| | | | dVdx′ = Dict(:volume=&gt;-150.0)
| | | Solving ω = 100.0
| | | | V = 20000.0
| | | | dVdx′ = Dict(:volume=&gt;-150.0)
| | | Adding cut : 75 volume_out + cost_to_go ≥ 13750.0
| | Visiting node 2
| | | Solving ω = 0.0
| | | | V = 36250.0
| | | | dVdx′ = Dict(:volume=&gt;-150.0)
| | | Solving ω = 50.0
| | | | V = 28750.0
| | | | dVdx′ = Dict(:volume=&gt;-150.0)
| | | Solving ω = 100.0
| | | | V = 21250.0
| | | | dVdx′ = Dict(:volume=&gt;-150.0)
| | | Adding cut : 150 volume_out + cost_to_go ≥ 28749.999999999996
| | Visiting node 3
| | | Solving ω = 0.0
| | | | V = 43750.0
| | | | dVdx′ = Dict(:volume=&gt;-150.0)
| | | Solving ω = 50.0
| | | | V = 36250.0
| | | | dVdx′ = Dict(:volume=&gt;-150.0)
| | | Solving ω = 100.0
| | | | V = 28749.999999999996
| | | | dVdx′ = Dict(:volume=&gt;-150.0)
| | | Adding cut : 75 volume_out + cost_to_go ≥ 18125.0
| | Visiting node 2
| | | Solving ω = 0.0
| | | | V = 33125.0
| | | | dVdx′ = Dict(:volume=&gt;-150.0)
| | | Solving ω = 50.0
| | | | V = 25625.0
| | | | dVdx′ = Dict(:volume=&gt;-150.0)
| | | Solving ω = 100.0
| | | | V = 18125.0
| | | | dVdx′ = Dict(:volume=&gt;-150.0)
| | | Adding cut : 150 volume_out + cost_to_go ≥ 33125.0
| | Visiting node 1
| | | Solving ω = 0.0
| | | | V = 33125.0
| | | | dVdx′ = Dict(:volume=&gt;-150.0)
| | | Solving ω = 50.0
| | | | V = 25625.0
| | | | dVdx′ = Dict(:volume=&gt;-150.0)
| | | Solving ω = 100.0
| | | | V = 18125.0
| | | | dVdx′ = Dict(:volume=&gt;-150.0)
| | | Adding cut : 150 volume_out + cost_to_go ≥ 40625.0
| Finished iteration
| | lower_bound = 15625.0
Starting iteration 2
| Forward Pass
| | Visiting node 1
| | | ω = 50.0
| | | x = Dict(:volume=&gt;200.0)
| | | x′ = Dict(:volume=&gt;200.0)
| | | C(x, u, ω) = 5000.0
| | Visiting node 2
| | | ω = 0.0
| | | x = Dict(:volume=&gt;200.0)
| | | x′ = Dict(:volume=&gt;200.0)
| | | C(x, u, ω) = 15000.0
| | Visiting node 3
| | | ω = 0.0
| | | x = Dict(:volume=&gt;200.0)
| | | x′ = Dict(:volume=&gt;50.0)
| | | C(x, u, ω) = 0.0
| | Visiting node 2
| | | ω = 100.0
| | | x = Dict(:volume=&gt;50.0)
| | | x′ = Dict(:volume=&gt;150.0)
| | | C(x, u, ω) = 15000.0
| | Visiting node 3
| | | ω = 0.0
| | | x = Dict(:volume=&gt;150.0)
| | | x′ = Dict(:volume=&gt;0.0)
| | | C(x, u, ω) = 0.0
| Backward pass
| | Visiting node 3
| | | Solving ω = 0.0
| | | | V = 48125.0
| | | | dVdx′ = Dict(:volume=&gt;-150.0)
| | | Solving ω = 50.0
| | | | V = 40625.0
| | | | dVdx′ = Dict(:volume=&gt;-150.0)
| | | Solving ω = 100.0
| | | | V = 33125.0
| | | | dVdx′ = Dict(:volume=&gt;-150.0)
| | | Adding cut : 75 volume_out + cost_to_go ≥ 20312.5
| | Visiting node 2
| | | Solving ω = 0.0
| | | | V = 20312.5
| | | | dVdx′ = Dict(:volume=&gt;-75.0)
| | | Solving ω = 50.0
| | | | V = 16562.5
| | | | dVdx′ = Dict(:volume=&gt;-75.0)
| | | Solving ω = 100.0
| | | | V = 12812.5
| | | | dVdx′ = Dict(:volume=&gt;-75.0)
| | | Adding cut : 75 volume_out + cost_to_go ≥ 27812.499999999996
| | Visiting node 3
| | | Solving ω = 0.0
| | | | V = 40625.0
| | | | dVdx′ = Dict(:volume=&gt;-150.0)
| | | Solving ω = 50.0
| | | | V = 34583.33333333333
| | | | dVdx′ = Dict(:volume=&gt;-100.0)
| | | Solving ω = 100.0
| | | | V = 29583.333333333332
| | | | dVdx′ = Dict(:volume=&gt;-100.0)
| | | Adding cut : 58.33333333333333 volume_out + cost_to_go ≥ 20381.94444444444
| | Visiting node 2
| | | Solving ω = 0.0
| | | | V = 17465.277777777774
| | | | dVdx′ = Dict(:volume=&gt;-58.3333)
| | | Solving ω = 50.0
| | | | V = 14548.61111111111
| | | | dVdx′ = Dict(:volume=&gt;-58.3333)
| | | Solving ω = 100.0
| | | | V = 11631.944444444442
| | | | dVdx′ = Dict(:volume=&gt;-58.3333)
| | | Adding cut : 58.33333333333333 volume_out + cost_to_go ≥ 26215.277777777774
| | Visiting node 1
| | | Solving ω = 0.0
| | | | V = 24583.33333333333
| | | | dVdx′ = Dict(:volume=&gt;-100.0)
| | | Solving ω = 50.0
| | | | V = 20381.94444444444
| | | | dVdx′ = Dict(:volume=&gt;-58.3333)
| | | Solving ω = 100.0
| | | | V = 17465.277777777774
| | | | dVdx′ = Dict(:volume=&gt;-58.3333)
| | | Adding cut : 72.22222222222221 volume_out + cost_to_go ≥ 35254.62962962962
| Finished iteration
| | lower_bound = 25810.185185185175
Starting iteration 3
| Forward Pass
| | Visiting node 1
| | | ω = 0.0
| | | x = Dict(:volume=&gt;200.0)
| | | x′ = Dict(:volume=&gt;200.0)
| | | C(x, u, ω) = 7500.0
| | Visiting node 2
| | | ω = 100.0
| | | x = Dict(:volume=&gt;200.0)
| | | x′ = Dict(:volume=&gt;150.0)
| | | C(x, u, ω) = 0.0
| | Visiting node 3
| | | ω = 50.0
| | | x = Dict(:volume=&gt;150.0)
| | | x′ = Dict(:volume=&gt;50.0)
| | | C(x, u, ω) = 0.0
| Backward pass
| | Visiting node 3
| | | Solving ω = 0.0
| | | | V = 40624.999999999985
| | | | dVdx′ = Dict(:volume=&gt;-150.0)
| | | Solving ω = 50.0
| | | | V = 34583.33333333332
| | | | dVdx′ = Dict(:volume=&gt;-100.0)
| | | Solving ω = 100.0
| | | | V = 29583.333333333325
| | | | dVdx′ = Dict(:volume=&gt;-100.0)
| | | Adding cut : 58.333333333333314 volume_out + cost_to_go ≥ 20381.944444444438
| | Visiting node 2
| | | Solving ω = 0.0
| | | | V = 20381.944444444438
| | | | dVdx′ = Dict(:volume=&gt;-58.3333)
| | | Solving ω = 50.0
| | | | V = 17465.277777777774
| | | | dVdx′ = Dict(:volume=&gt;-58.3333)
| | | Solving ω = 100.0
| | | | V = 14548.611111111106
| | | | dVdx′ = Dict(:volume=&gt;-58.3333)
| | | Adding cut : 58.33333333333333 volume_out + cost_to_go ≥ 26215.277777777774
| | Visiting node 1
| | | Solving ω = 0.0
| | | | V = 24583.33333333333
| | | | dVdx′ = Dict(:volume=&gt;-100.0)
| | | Solving ω = 50.0
| | | | V = 20381.94444444444
| | | | dVdx′ = Dict(:volume=&gt;-58.3333)
| | | Solving ω = 100.0
| | | | V = 17465.277777777774
| | | | dVdx′ = Dict(:volume=&gt;-58.3333)
| | | Adding cut : 72.22222222222221 volume_out + cost_to_go ≥ 35254.62962962962
| Finished iteration
| | lower_bound = 25810.185185185175
Upper bound = 27233.33333333333 ± 5483.231215826824</pre><p>Success! We trained a policy for an infinite horizon multistage stochastic program using stochastic dual dynamic programming. Note how some of the forward passes are different lengths!</p><pre><code class="language-julia">decision_rule(
    model.nodes[3];
    incoming_state = Dict(:volume =&gt; 100.0),
    random_variable = 10.0,
)</code></pre><pre class="documenter-example-output">Dict{Symbol,Float64} with 8 entries:
  :volume_out         =&gt; 0.0
  :demand_constraint  =&gt; 150.0
  :hydro_spill        =&gt; 0.0
  :inflow             =&gt; 10.0
  :volume_in          =&gt; 100.0
  :thermal_generation =&gt; 40.0
  :hydro_generation   =&gt; 110.0
  :cost_to_go         =&gt; 20381.9</pre><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../13_integrality/">« Advanced III: integrality</a><a class="docs-footer-nextpage" href="../../guides/add_a_multidimensional_state_variable/">Add a multi-dimensional state variable »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Thursday 7 January 2021 03:28">Thursday 7 January 2021</span>. Using Julia version 1.0.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
