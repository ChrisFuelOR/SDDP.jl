<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Expert I: Pedagogical SDDP · SDDP.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="SDDP.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit">SDDP.jl</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><input class="collapse-toggle" id="menuitem-2" type="checkbox" checked/><label class="tocitem" for="menuitem-2"><span class="docs-label">Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../01_first_steps/">Basic I: first steps</a></li><li><a class="tocitem" href="../02_adding_uncertainty/">Basic II: adding uncertainty</a></li><li><a class="tocitem" href="../03_objective_uncertainty/">Basic III: objective uncertainty</a></li><li><a class="tocitem" href="../04_markov_uncertainty/">Basic IV: Markov uncertainty</a></li><li><a class="tocitem" href="../05_plotting/">Basic V: plotting</a></li><li><a class="tocitem" href="../06_warnings/">Basic VI: words of warning</a></li><li><a class="tocitem" href="../11_objective_states/">Advanced I: objective states</a></li><li><a class="tocitem" href="../12_belief_states/">Advanced II: belief states</a></li><li><a class="tocitem" href="../13_integrality/">Advanced III: integrality</a></li><li class="is-active"><a class="tocitem" href>Expert I: Pedagogical SDDP</a><ul class="internal"><li><a class="tocitem" href="#Theory"><span>Theory</span></a></li><li><a class="tocitem" href="#Implementation-2"><span>Implementation</span></a></li></ul></li></ul></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox"/><label class="tocitem" for="menuitem-3"><span class="docs-label">How-to guides</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../guides/add_a_multidimensional_state_variable/">Add a multi-dimensional state variable</a></li><li><a class="tocitem" href="../../guides/add_a_risk_measure/">Add a risk measure</a></li><li><a class="tocitem" href="../../guides/add_multidimensional_noise_Terms/">Add multi-dimensional noise terms</a></li><li><a class="tocitem" href="../../guides/add_noise_in_the_constraint_matrix/">Add noise in the constraint matrix</a></li><li><a class="tocitem" href="../../guides/choose_a_stopping_rule/">Choose a stopping rule</a></li><li><a class="tocitem" href="../../guides/create_a_general_policy_graph/">Create a general policy graph</a></li><li><a class="tocitem" href="../../guides/debug_a_model/">Debug a model</a></li><li><a class="tocitem" href="../../guides/improve_computational_performance/">Improve computational performance</a></li><li><a class="tocitem" href="../../guides/simulate_using_a_different_sampling_scheme/">Simulate using a different sampling scheme</a></li><li><a class="tocitem" href="../../guides/upgrade_from_the_old_sddp/">Upgrade from the old SDDP.jl</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">Examples</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../examples/simple_hydro_thermal/">Hydro-thermal scheduling</a></li><li><a class="tocitem" href="../../examples/the_farmers_problem/">The farmer&#39;s problem</a></li></ul></li><li><a class="tocitem" href="../../apireference/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href>Expert I: Pedagogical SDDP</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Expert I: Pedagogical SDDP</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/odow/SDDP.jl/blob/master/docs/src/tutorial/22_pedagogical_sddp.jl" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Expert-I:-Pedagogical-SDDP"><a class="docs-heading-anchor" href="#Expert-I:-Pedagogical-SDDP">Expert I: Pedagogical SDDP</a><a id="Expert-I:-Pedagogical-SDDP-1"></a><a class="docs-heading-anchor-permalink" href="#Expert-I:-Pedagogical-SDDP" title="Permalink"></a></h1><p>In this tutorial we walk through a simplified implementation of stochastic dual dynamic programming to explain the key concepts.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>If you haven&#39;t already, go read <a href="../01_first_steps/#Basic-I:-first-steps">Basic I: first steps</a>, since it introduces much necessary background theory, and <a href="../02_adding_uncertainty/#Basic-II:-adding-uncertainty">Basic II: adding uncertainty</a>, since it defines the same example we will solve in our implementation.</p></div></div><p>This tutorial uses the following packages. For clarity, we call <code>import PackageName</code> so that we must prefix <code>PackageName.</code> to all functions and structs provided by that package. Everything not prefixed is either part of base Julia, or we wrote it.</p><pre><code class="language-julia">import ForwardDiff
import GLPK
import JuMP
import Statistics</code></pre><h2 id="Theory"><a class="docs-heading-anchor" href="#Theory">Theory</a><a id="Theory-1"></a><a class="docs-heading-anchor-permalink" href="#Theory" title="Permalink"></a></h2><h3 id="Preliminaries:-Kelley&#39;s-cutting-plane-algorithm"><a class="docs-heading-anchor" href="#Preliminaries:-Kelley&#39;s-cutting-plane-algorithm">Preliminaries: Kelley&#39;s cutting plane algorithm</a><a id="Preliminaries:-Kelley&#39;s-cutting-plane-algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#Preliminaries:-Kelley&#39;s-cutting-plane-algorithm" title="Permalink"></a></h3><p>Kelley&#39;s cutting plane algorithm is an iterative method for minimizing convex functions. Given a convex function <span>$f(x)$</span>, Kelley&#39;s constructs an under-approximation of the function at the minimum by a set of first-order Taylor series approximations (called <strong>cuts</strong>) constructed at a set of <span>$K$</span> points <span>$k = 1,\ldots,K$</span>:</p><p class="math-container">\[\begin{aligned}
f^K = \min\limits_{\theta, x} \;\; &amp; \theta\\
&amp; \theta \ge f(x_k) + \frac{df}{dx}\left(x_k\right) \cdot (x - x_k),\quad k=1,\ldots,K\\
&amp; \theta \ge M,
\end{aligned}\]</p><p>where <span>$M$</span> is a sufficiently large negative number that is a lower bound for <span>$f$</span> over the domain of <span>$x$</span>.</p><p>As more cuts are added:</p><p class="math-container">\[\lim_{K \rightarrow \infty} f^K = \min\limits_{x} f(x)\]</p><h4 id="Bounds"><a class="docs-heading-anchor" href="#Bounds">Bounds</a><a id="Bounds-1"></a><a class="docs-heading-anchor-permalink" href="#Bounds" title="Permalink"></a></h4><p>By convexity, <span>$f^K \le f(x)$</span> for all <span>$x$</span>. Thus, if <span>$x^*$</span> is a minimizer of <span>$f$</span>, then at any point in time we can construct a lower bound for <span>$f(x^*)$</span> by solving <span>$f^K$</span>.</p><p>Moreover, since any feasible point is an upper bound, we can use the primal solution <span>$x^K$</span> returned by solving <span>$f^K$</span> to evaluate <span>$f(x_K)$</span> to generate an upper bound.</p><p>Therefore, <span>$f(x^*) \in [f^K, f(x_K)]$</span>.</p><h4 id="Implementation"><a class="docs-heading-anchor" href="#Implementation">Implementation</a><a id="Implementation-1"></a><a class="docs-heading-anchor-permalink" href="#Implementation" title="Permalink"></a></h4><p>Here is pseudo-code fo the algorithm:</p><ol><li>Take as input a function <span>$f$</span> and a iteration limit <span>$K_{max}$</span>. Set <span>$K = 0$</span>, and initialize <span>$f^K$</span></li><li>Solve <span>$f^K$</span> to obtain a candidate solution <span>$x_{K+1}$</span>.</li><li>Add a cut <span>$\theta \ge f(x_{K+1}) + \frac{df}{dx}\left(x_{K+1}\right)^\top (x - x_{K+1})$</span> to form <span>$f^{K+1}$</span>.</li><li>Increment <span>$K$</span></li><li>If <span>$K = K_{max}$</span> STOP, otherwise, go to step 2.</li></ol><p>And here&#39;s a complete implementation:</p><pre><code class="language-julia">function kelleys_cutting_plane(
    # The function to be minimized.
    f::Function,
    # The gradient of `f`. By default, we use automatic differentiation to
    # compute the gradient of f so the user doesn&#39;t have to!
    dfdx::Function = x -&gt; ForwardDiff.gradient(f, x);
    # The number of arguments to `f`.
    input_dimension::Int,
    # A lower bound for the function `f` over its domain.
    lower_bound::Float64,
    # The number of iterations to run Kelley&#39;s algorithm for before stopping.
    iteration_limit::Int,
)
    # Step (1):
    K = 0
    model = JuMP.Model(GLPK.Optimizer)
    JuMP.@variable(model, θ &gt;= lower_bound)
    JuMP.@variable(model, x[1:input_dimension])
    JuMP.@objective(model, Min, θ)
    while true
        # Step (2)
        JuMP.optimize!(model)
        x_k = JuMP.value.(x)
        # Step (3):
        c = JuMP.@constraint(model, θ &gt;= f(x_k) + dfdx(x_k)&#39; * (x .- x_k))
        # Step (4):
        K = K + 1
        # Step (5):
        if K == iteration_limit
            break
        end
    end
    θ_K, x_K = JuMP.value(θ), JuMP.value.(x)
    println(&quot;Found solution:&quot;)
    println(&quot;  x_K   = &quot;, x_K)
    println(&quot;  f(x*) ∈ [&quot;, θ_K, &quot;, &quot;, f(x_K), &quot;]&quot;)
    return
end</code></pre><pre class="documenter-example-output">kelleys_cutting_plane (generic function with 2 methods)</pre><p>Let&#39;s run our algorithm to see what happens:</p><pre><code class="language-julia">kelleys_cutting_plane(
    input_dimension = 2,
    lower_bound = 0.0,
    iteration_limit = 20,
) do x
    return (x[1] - 1)^2 + (x[2] + 2)^2
end</code></pre><pre class="documenter-example-output">Found solution:
  x_K   = [0.997655, -2.00352]
  f(x*) ∈ [0.0, 1.7896182563008953e-5]</pre><h3 id="Approximating-the-cost-to-go-term"><a class="docs-heading-anchor" href="#Approximating-the-cost-to-go-term">Approximating the cost-to-go term</a><a id="Approximating-the-cost-to-go-term-1"></a><a class="docs-heading-anchor-permalink" href="#Approximating-the-cost-to-go-term" title="Permalink"></a></h3><p>In <a href="../01_first_steps/#Basic-I:-first-steps">Basic I: first steps</a>, we discussed how you could formulate an optimal policy to a multistage stochastic program using the dynamic programming recursion:</p><p class="math-container">\[\begin{aligned}
V_i(x, \omega) = \min\limits_{\bar{x}, x^\prime, u} \;\; &amp; C_i(\bar{x}, u, \omega) + \mathbb{E}_{j \in i^+, \varphi \in \Omega_j}[V_j(x^\prime, \varphi)]\\
&amp; x^\prime = T_i(\bar{x}, u, \omega) \\
&amp; u \in U_i(\bar{x}, \omega) \\
&amp; \bar{x} = x
\end{aligned}\]</p><p>where our decision rule, <span>$\pi_i(x, \omega)$</span>, solves this optimization problem and returns a <span>$u^*$</span> corresponding to an optimal solution. Moreover, we alluded to the fact that the cost-to-go term (the nasty recursive expectation) makes this problem intractable to solve.</p><p>However, if, excluding the cost-to-go term, <span>$V_i(x, \omega)$</span> can be formulated as a linear program (this also works for convex programs, but the math is more involved), then we can make some progress.</p><p>First, notice that <span>$x$</span> only appears as a right-hand side term of <span>$V_i$</span>. Therefore, <span>$V_i(x, \cdot)$</span> is convex with respect to <span>$x$</span> for fixed <span>$\omega$</span>. Moreover, the reduced cost of the decision variable <span>$\bar{x}$</span> is a subgradient of the function <span>$V_i$</span> with respect to <span>$x$</span>! (This is one reason why we add the <span>$\bar{x}$</span> and the fishing constraint <span>$\bar{x} = x$</span>.)</p><p>Second, a convex combination of convex functions is also convex, so the cost-to-go term is a convex function of <span>$x^\prime$</span>.</p><p>Stochastic dual dynamic programming converts this problem into a tractable form by applying Kelley&#39;s cutting plane algorithm to the cost-to-go term:</p><p class="math-container">\[\begin{aligned}
V_i^K(x, \omega) = \min\limits_{\bar{x}, x^\prime, u} \;\; &amp; C_i(\bar{x}, u, \omega) + \theta\\
&amp; x^\prime = T_i(\bar{x}, u, \omega) \\
&amp; u \in U_i(\bar{x}, \omega) \\
&amp; \bar{x} = x \\
&amp; \theta \ge \mathbb{E}_{j \in i^+, \varphi \in \Omega_j}\left[V_j^k(x^\prime_k, \varphi) + \frac{dV_j^k}{dx^\prime}\left(x^\prime_k, \varphi\right)^\top (x^\prime - x^\prime_k)\right],\quad k=1,\ldots,K \\
&amp; \theta \ge M
\end{aligned}\]</p><p>All we need now is a way of generating these cutting planes in an iterative manner.</p><h3 id="The-forward-pass"><a class="docs-heading-anchor" href="#The-forward-pass">The forward pass</a><a id="The-forward-pass-1"></a><a class="docs-heading-anchor-permalink" href="#The-forward-pass" title="Permalink"></a></h3><h3 id="The-backward-pass"><a class="docs-heading-anchor" href="#The-backward-pass">The backward pass</a><a id="The-backward-pass-1"></a><a class="docs-heading-anchor-permalink" href="#The-backward-pass" title="Permalink"></a></h3><h2 id="Implementation-2"><a class="docs-heading-anchor" href="#Implementation-2">Implementation</a><a class="docs-heading-anchor-permalink" href="#Implementation-2" title="Permalink"></a></h2><p>For this implementation of SDDP, we&#39;re going to try and keep things as simple as possible. This is very much a &quot;vanilla&quot; version of SDDP; it doesn&#39;t have (m)any fancy computational tricks that you need to code a performant or stable version that will work on realistic instances.</p><p>In the interests of brevity, we will also include minimal error checking. Think about all the different ways you could break this code!</p><h3 id="Structs"><a class="docs-heading-anchor" href="#Structs">Structs</a><a id="Structs-1"></a><a class="docs-heading-anchor-permalink" href="#Structs" title="Permalink"></a></h3><p>The first struct we are going to use is a <code>State</code> struct that will wrap an incoming and outgoing state variable.</p><pre><code class="language-julia">struct State
    in::JuMP.VariableRef
    out::JuMP.VariableRef
end</code></pre><p>Next, we need a struct to wrap all of the uncertainty within a node.</p><pre><code class="language-julia">struct Uncertainty
    parameterize::Function
    Ω::Vector{Any}
    P::Vector{Float64}
end</code></pre><p><code>parameterize</code> is a function, which takes a realization of the random variable <span>$\omega\in\Omega$</span> and updates the subproblem accordingly. The finite discrete random variable is defined by the vectors <code>Ω</code> and <code>P</code>, so that the random variable takes the value <code>Ω[i]</code> with probability <code>P[i]</code>. As such, <code>P</code> should sum to 1. (We don&#39;t check this here, but we should; we do in SDDP.jl.)</p><p>It&#39;s also going to be useful to have a function that samples a realization of the random variable defined by <code>Ω</code> and <code>P</code>:</p><pre><code class="language-julia">function sample_uncertainty(uncertainty::Uncertainty)
    r = rand()
    for (p, ω) in zip(uncertainty.P, uncertainty.Ω)
        if r &lt;= p
            return ω
        end
        r -= p
    end
    error(&quot;We should never get here because P should sum to 1.0.&quot;)
end</code></pre><pre class="documenter-example-output">sample_uncertainty (generic function with 1 method)</pre><p>You should be able to work out what is going on. <code>rand()</code> samples a uniform random variable in <code>[0, 1)</code>.</p><p>Now we have two building blocks, we can declare the structure of each node.</p><pre><code class="language-julia">struct Node
    subproblem::JuMP.Model
    states::Dict{Symbol, State}
    uncertainty::Uncertainty
    cost_to_go::JuMP.VariableRef
end</code></pre><p><code>subproblem</code> is going to be the JuMP model that we build at each node. <code>states</code> is a dictionary that maps a symbolic name of a state variable to a <code>State</code> object wrapping the incoming and outgoing state variables in <code>subproblem</code>. <code>uncertainty</code> is an <code>Uncertainty</code> object described above, and <code>cost_to_go</code> is a JuMP variable that approximates the cost-to-go term.</p><p>Finally, out simplified policy graph is just a vector of nodes.</p><pre><code class="language-julia">struct LinearPolicyGraph
    nodes::Vector{Node}
end</code></pre><p>We also define a nice <code>show</code> method so that we don&#39;t accidentally print a large amount of information to the screen.</p><pre><code class="language-julia">function Base.show(io::IO, model::LinearPolicyGraph)
    return print(io, &quot;A policy graph with $(length(model.nodes)) nodes&quot;)
end</code></pre><h3 id="Interface-functions"><a class="docs-heading-anchor" href="#Interface-functions">Interface functions</a><a id="Interface-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Interface-functions" title="Permalink"></a></h3><p>Now we have some basic types, let&#39;s implment some functions so that the user can create a model.</p><p>First, we need an exmaple of a function that the user will provide. Like SDDP.jl, this takes an empty <code>subproblem</code>, and a node index, in this case <code>t::Int</code>. You could change this function to change the model, or define a new one later in the code.</p><pre><code class="language-julia">function subproblem_builder(subproblem::JuMP.Model, t::Int)
    # Define the state variables. Note how we fix the incoming state to the
    # initial state variable regardless of `t`! This isn&#39;t strictly necessary;
    # it only matters that we do it for the first node.
    JuMP.@variable(subproblem, volume_in == 200)
    JuMP.@variable(subproblem, 0 &lt;= volume_out &lt;= 200)
    states = Dict(:volume =&gt; State(volume_in, volume_out))
    # Define the control variables.
    JuMP.@variables(subproblem, begin
        thermal_generation &gt;= 0
        hydro_generation   &gt;= 0
        hydro_spill        &gt;= 0
        inflow
    end)
    # Define the constraints
    JuMP.@constraints(subproblem, begin
        volume_out == volume_in + inflow - hydro_generation - hydro_spill
        demand_constraint, thermal_generation + hydro_generation == 150.0
    end)
    # Define the objective for each stage `t`. Note that we can use `t` as an
    # index for t = 1, 2, 3.
    fuel_cost = [50.0, 100.0, 150.0]
    JuMP.@objective(subproblem, Min, fuel_cost[t] * thermal_generation)
    # Finally, we define the uncertainty object. Because this is a simplified
    # implementation of SDDP, we shall politely ask the user to only modify the
    # constraints, and not the objective function! (Not that it changes the
    # algorithm, we just have to add more information to keep track of things.)
    uncertainty = Uncertainty([0.0, 50.0, 100.0], [1 / 3, 1 / 3, 1 / 3]) do ω
        JuMP.fix(inflow, ω)
    end
    return states, uncertainty
end</code></pre><pre class="documenter-example-output">subproblem_builder (generic function with 1 method)</pre><p>If you&#39;ve read <a href="../02_adding_uncertainty/#Basic-II:-adding-uncertainty">Basic II: adding uncertainty</a>, this example should be familiar. You can probably see how some of the SDDP.jl functionality like <a href="../../apireference/#SDDP.@stageobjective"><code>@stageobjective</code></a> and <a href="../../apireference/#SDDP.parameterize"><code>SDDP.parameterize</code></a> help smooth some of the usability issues like needing to construct both the incoming and outgoing state variables, or needing to explicitly <code>return states, uncertainty</code>.</p><p>The next function we need to define is the analog of <a href="../../apireference/#SDDP.LinearPolicyGraph"><code>SDDP.LinearPolicyGraph</code></a>. It should be pretty readable.</p><pre><code class="language-julia">function LinearPolicyGraph(
    subproblem_builder::Function;
    stages::Int,
    lower_bound::Float64,
    optimizer,
)
    nodes = Node[]
    for t = 1:stages
        # Create a model.
        model = JuMP.Model(optimizer)
        # Use the provided function to build out each subproblem. The user&#39;s
        # function returns a dictionary mapping `Symbol`s to `State` objects,
        # and an `Uncertainty` object.
        states, uncertainty = subproblem_builder(model, t)
        # Now add the cost-to-go terms:
        JuMP.@variable(model, cost_to_go &gt;= lower_bound)
        obj = JuMP.objective_function(model)
        JuMP.@objective(model, Min, obj + cost_to_go)
        # In the final stage, the cost-to-go is 0.0.
        if t == stages
            JuMP.fix(cost_to_go, 0.0; force = true)
        end
        push!(nodes, Node(model, states, uncertainty, cost_to_go))
    end
    return LinearPolicyGraph(nodes)
end</code></pre><pre class="documenter-example-output">Main.ex-22_pedagogical_sddp.LinearPolicyGraph</pre><h3 id="The-forward-pass-2"><a class="docs-heading-anchor" href="#The-forward-pass-2">The forward pass</a><a class="docs-heading-anchor-permalink" href="#The-forward-pass-2" title="Permalink"></a></h3><p>Now we&#39;re ready to code the forward pass. It takes a <code>::LinearPolicyGraph</code>, and returns a tuple of two things: a vector of the outgoing state variables visited, and a <code>Float64</code> of the cumulative stage costs that were incurred along the forward pass.</p><pre><code class="language-julia">function forward_pass(model::LinearPolicyGraph, io::IO = stdout)
    println(io, &quot;| Forward Pass&quot;)
    # First, get the value of the state at the root node (e.g., x_R).
    incoming_state = Dict(
        k =&gt; JuMP.fix_value(v.in) for (k, v) in model.nodes[1].states
    )
    # `simulation_cost` is an accumlator that is going to sum the stage-costs
    # incurred over the forward pass.
    simulation_cost = 0.0
    # We also need to record the outgoing state variables so we can pass them
    # to the backward pass.
    outgoing_states = Dict{Symbol, Float64}[]
    # Now&#39;s the meat of the forward pass: loop through each of the nodes
    for (t, node) in enumerate(model.nodes)
        println(io, &quot;| | Visiting node $(t)&quot;)
        # Sample the uncertainty:
        ω = sample_uncertainty(node.uncertainty)
        println(io, &quot;| |  ω = &quot;, ω)
        # Before parameterizing the subproblem using the user-provided
        # function:
        node.uncertainty.parameterize(ω)
        println(io, &quot;| |  x = &quot;, incoming_state)
        # Update the incoming state variable:
        for (k, v) in incoming_state
            JuMP.fix(node.states[k].in, v; force = true)
        end
        # Now solve the subproblem and check we found an optimal solution:
        JuMP.optimize!(node.subproblem)
        if JuMP.termination_status(node.subproblem) != JuMP.MOI.OPTIMAL
            error(&quot;Something went terribly wrong!&quot;)
        end
        # Compute the outgoing state variables, and save them in
        # `outgoing_states`:
        outgoing_state = Dict(k =&gt; JuMP.value(v.out) for (k, v) in node.states)
        push!(outgoing_states, outgoing_state)
        println(io, &quot;| |  x′ = &quot;, outgoing_state)
        # We also need to compute the stage cost to add to our
        # `simulation_cost` accumulator:
        stage_cost = JuMP.objective_value(node.subproblem) - JuMP.value(node.cost_to_go)
        simulation_cost += stage_cost
        println(io, &quot;| |  C(x, u, ω) = &quot;, stage_cost)
        # As a final step, set the outgoing state of stage t and the incoming
        # state of stage t + 1:
        incoming_state = outgoing_state
    end
    return outgoing_states, simulation_cost
end</code></pre><pre class="documenter-example-output">forward_pass (generic function with 2 methods)</pre><h3 id="The-backward-pass-2"><a class="docs-heading-anchor" href="#The-backward-pass-2">The backward pass</a><a class="docs-heading-anchor-permalink" href="#The-backward-pass-2" title="Permalink"></a></h3><p>We&#39;re now ready to code the backward pass. This is going to take a <code>::LinearPolicyGraph</code> object, and a vector of outgoing states from the forward pass. It returns a <code>Float64</code> that is a valid lower bound for the objective of the multistage stochastic program.</p><pre><code class="language-julia">function backward_pass(
    model::LinearPolicyGraph,
    outgoing_states::Vector{Dict{Symbol, Float64}},
)
    println(&quot;| Backward pass&quot;)
    # For the backward pass, we walk back up the nodes, from the final
    # node to the second (we will solve the first node after this loop).
    for t = length(outgoing_states):-1:2
        println(&quot;| | Visiting node $(t)&quot;)
        # At each step in the backward pass, we are going to solve problems in
        # stage t, but add cuts to stage t - 1. Thus, we need:
        node_t = model.nodes[t]
        node_t1 = model.nodes[t - 1]
        # First, fix the incoming state variables of stage t to the value of
        # the outgoing state variables in stage t - 1.
        for (k, v) in outgoing_states[t - 1]
            JuMP.fix(node_t.states[k].in, v; force = true)
        end
        # Then, create an empty affine expression that we will use to build
        # up the cut expression.
        cut_expression = JuMP.AffExpr(0.0)
        # Now for each possible realization of the uncertainty, solve the
        # stage t subproblem, and add `p * [y + λᵀ(x - x_k)]` to the cut
        # expression. (See the Theory section above is this isn&#39;t obvious why.)
        for (p, ω) in zip(node_t.uncertainty.P, node_t.uncertainty.Ω)
            println(&quot;| | | Solving ω = &quot;, ω)
            node_t.uncertainty.parameterize(ω)
            JuMP.optimize!(node_t.subproblem)
            y = JuMP.objective_value(node_t.subproblem)
            println(&quot;| | |  y = &quot;, y)
            λ = Dict(k =&gt; JuMP.reduced_cost(v.in) for (k, v) in node_t.states)
            println(&quot;| | |  λ = &quot;, λ)
            cut_expression += p * JuMP.@expression(
                node_t1.subproblem,
                y + sum(
                    λ[k] * (x.out - outgoing_states[t - 1][k])
                    for (k, x) in node_t1.states
                ),
            )
        end
        # And then refine the cost-to-go variable by adding a cut that is the
        # expectation of the cuts computed in the step above.
        c = JuMP.@constraint(
            node_t1.subproblem, node_t1.cost_to_go &gt;= cut_expression
        )
        println(&quot;| | | Adding cut : &quot;, c)
    end
    # Finally, compute a lower bound for the problem by evaluating the
    # first-stage subproblem.
    first_node = model.nodes[1]
    lower_bound = 0.0
    for (p, ω) in zip(first_node.uncertainty.P, first_node.uncertainty.Ω)
        first_node.uncertainty.parameterize(ω)
        JuMP.optimize!(first_node.subproblem)
        lower_bound += p * JuMP.objective_value(first_node.subproblem)
    end
    return lower_bound
end</code></pre><pre class="documenter-example-output">backward_pass (generic function with 1 method)</pre><p>Thirdly, we need a function to simulate the policy. This is going be very simple. It doesn&#39;t have an bells and whistles like being able to record the control variables.</p><pre><code class="language-julia">function simulate(model::LinearPolicyGraph; replications::Int)
    # Pipe the output to `devnull` so we don&#39;t print too much!
    simulations = [forward_pass(model, devnull) for _ = 1:replications]
    z = [s[2] for s in simulations]
    μ  = Statistics.mean(z)
    tσ = 1.96 * Statistics.std(z) / sqrt(replications)
    println(&quot;Upper bound = $(μ) ± $(tσ)&quot;)
    return simulations
end</code></pre><pre class="documenter-example-output">simulate (generic function with 1 method)</pre><p>Finally, the <code>train</code> loop of SDDP just applies the forward and backward passes iteratively, followed by a final simulation to compute the upper bound confidence interval.</p><pre><code class="language-julia">function train(
    model::LinearPolicyGraph;
    iteration_limit::Int,
    replications::Int,
)
    for i = 1:iteration_limit
        println(&quot;Starting iteration $(i)&quot;)
        outgoing_states, simulation = forward_pass(model)
        lower_bound = backward_pass(model, outgoing_states)
        println(&quot;| Finished iteration&quot;)
        println(&quot;| | simulation = &quot;, simulation)
        println(&quot;| | lower_bound = &quot;, lower_bound)
    end
    simulate(model; replications = replications)
    return
end</code></pre><pre class="documenter-example-output">train (generic function with 1 method)</pre><h3 id="Example"><a class="docs-heading-anchor" href="#Example">Example</a><a id="Example-1"></a><a class="docs-heading-anchor-permalink" href="#Example" title="Permalink"></a></h3><p>First, create the model using the <code>subproblem_builder</code> function we defined earlier:</p><pre><code class="language-julia">model = LinearPolicyGraph(
    subproblem_builder;
    stages = 3,
    lower_bound = 0.0,
    optimizer = GLPK.Optimizer,
)</code></pre><pre class="documenter-example-output">A policy graph with 3 nodes</pre><p>Then, train a policy:</p><pre><code class="language-julia">train(model; iteration_limit = 3, replications = 100)</code></pre><pre class="documenter-example-output">Starting iteration 1
| Forward Pass
| | Visiting node 1
| |  ω = 0.0
| |  x = Dict(:volume=&gt;200.0)
| |  x′ = Dict(:volume=&gt;50.0)
| |  C(x, u, ω) = 0.0
| | Visiting node 2
| |  ω = 0.0
| |  x = Dict(:volume=&gt;50.0)
| |  x′ = Dict(:volume=&gt;0.0)
| |  C(x, u, ω) = 10000.0
| | Visiting node 3
| |  ω = 0.0
| |  x = Dict(:volume=&gt;0.0)
| |  x′ = Dict(:volume=&gt;0.0)
| |  C(x, u, ω) = 22500.0
| Backward pass
| | Visiting node 3
| | | Solving ω = 0.0
| | |  y = 22500.0
| | |  λ = Dict(:volume=&gt;-150.0)
| | | Solving ω = 50.0
| | |  y = 15000.0
| | |  λ = Dict(:volume=&gt;-150.0)
| | | Solving ω = 100.0
| | |  y = 7500.0
| | |  λ = Dict(:volume=&gt;-150.0)
| | | Adding cut : 150 volume_out + cost_to_go ≥ 15000.0
| | Visiting node 2
| | | Solving ω = 0.0
| | |  y = 22500.0
| | |  λ = Dict(:volume=&gt;-150.0)
| | | Solving ω = 50.0
| | |  y = 15000.0
| | |  λ = Dict(:volume=&gt;-150.0)
| | | Solving ω = 100.0
| | |  y = 10000.0
| | |  λ = Dict(:volume=&gt;-100.0)
| | | Adding cut : 133.33333333333331 volume_out + cost_to_go ≥ 22500.0
| Finished iteration
| | simulation = 32500.0
| | lower_bound = 3437.500000000001
Starting iteration 2
| Forward Pass
| | Visiting node 1
| |  ω = 0.0
| |  x = Dict(:volume=&gt;200.0)
| |  x′ = Dict(:volume=&gt;168.75)
| |  C(x, u, ω) = 5937.500000000001
| | Visiting node 2
| |  ω = 50.0
| |  x = Dict(:volume=&gt;168.75)
| |  x′ = Dict(:volume=&gt;100.0)
| |  C(x, u, ω) = 3125.0
| | Visiting node 3
| |  ω = 50.0
| |  x = Dict(:volume=&gt;100.0)
| |  x′ = Dict(:volume=&gt;0.0)
| |  C(x, u, ω) = 0.0
| Backward pass
| | Visiting node 3
| | | Solving ω = 0.0
| | |  y = 7500.0
| | |  λ = Dict(:volume=&gt;-150.0)
| | | Solving ω = 50.0
| | |  y = 0.0
| | |  λ = Dict(:volume=&gt;-150.0)
| | | Solving ω = 100.0
| | |  y = 0.0
| | |  λ = Dict(:volume=&gt;0.0)
| | | Adding cut : 100 volume_out + cost_to_go ≥ 12500.0
| | Visiting node 2
| | | Solving ω = 0.0
| | |  y = 10625.0
| | |  λ = Dict(:volume=&gt;-100.0)
| | | Solving ω = 50.0
| | |  y = 5625.0
| | |  λ = Dict(:volume=&gt;-100.0)
| | | Solving ω = 100.0
| | |  y = 625.0
| | |  λ = Dict(:volume=&gt;-100.0)
| | | Adding cut : 99.99999999999999 volume_out + cost_to_go ≥ 22499.999999999996
| Finished iteration
| | simulation = 9062.5
| | lower_bound = 7500.0
Starting iteration 3
| Forward Pass
| | Visiting node 1
| |  ω = 0.0
| |  x = Dict(:volume=&gt;200.0)
| |  x′ = Dict(:volume=&gt;200.0)
| |  C(x, u, ω) = 7500.0
| | Visiting node 2
| |  ω = 100.0
| |  x = Dict(:volume=&gt;200.0)
| |  x′ = Dict(:volume=&gt;125.0)
| |  C(x, u, ω) = 0.0
| | Visiting node 3
| |  ω = 50.0
| |  x = Dict(:volume=&gt;125.0)
| |  x′ = Dict(:volume=&gt;25.0)
| |  C(x, u, ω) = 0.0
| Backward pass
| | Visiting node 3
| | | Solving ω = 0.0
| | |  y = 3750.0
| | |  λ = Dict(:volume=&gt;-150.0)
| | | Solving ω = 50.0
| | |  y = 0.0
| | |  λ = Dict(:volume=&gt;0.0)
| | | Solving ω = 100.0
| | |  y = 0.0
| | |  λ = Dict(:volume=&gt;0.0)
| | | Adding cut : 50 volume_out + cost_to_go ≥ 7500.0
| | Visiting node 2
| | | Solving ω = 0.0
| | |  y = 7500.0
| | |  λ = Dict(:volume=&gt;-100.0)
| | | Solving ω = 50.0
| | |  y = 2500.0
| | |  λ = Dict(:volume=&gt;-100.0)
| | | Solving ω = 100.0
| | |  y = 0.0
| | |  λ = Dict(:volume=&gt;0.0)
| | | Adding cut : 66.66666666666666 volume_out + cost_to_go ≥ 16666.666666666664
| Finished iteration
| | simulation = 7500.0
| | lower_bound = 8333.333333333332
Upper bound = 8550.0 ± 934.8604173886067</pre><p>Success! We solved a multistage stochastic program using stochastic dual dynamic programming.</p><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../13_integrality/">« Advanced III: integrality</a><a class="docs-footer-nextpage" href="../../guides/add_a_multidimensional_state_variable/">Add a multi-dimensional state variable »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Wednesday 6 January 2021 22:52">Wednesday 6 January 2021</span>. Using Julia version 1.0.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
