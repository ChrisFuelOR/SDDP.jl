<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Expert I: Pedagogical SDDP · SDDP.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="SDDP.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit">SDDP.jl</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><input class="collapse-toggle" id="menuitem-2" type="checkbox" checked/><label class="tocitem" for="menuitem-2"><span class="docs-label">Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../01_first_steps/">Basic I: first steps</a></li><li><a class="tocitem" href="../02_adding_uncertainty/">Basic II: adding uncertainty</a></li><li><a class="tocitem" href="../03_objective_uncertainty/">Basic III: objective uncertainty</a></li><li><a class="tocitem" href="../04_markov_uncertainty/">Basic IV: Markov uncertainty</a></li><li><a class="tocitem" href="../05_plotting/">Basic V: plotting</a></li><li><a class="tocitem" href="../06_warnings/">Basic VI: words of warning</a></li><li><a class="tocitem" href="../11_objective_states/">Advanced I: objective states</a></li><li><a class="tocitem" href="../12_belief_states/">Advanced II: belief states</a></li><li><a class="tocitem" href="../13_integrality/">Advanced III: integrality</a></li><li class="is-active"><a class="tocitem" href>Expert I: Pedagogical SDDP</a><ul class="internal"><li><a class="tocitem" href="#Preliminaries:-Kelley&#39;s-cutting-plane-algorithm"><span>Preliminaries: Kelley&#39;s cutting plane algorithm</span></a></li><li><a class="tocitem" href="#Preliminaries:-Approximating-the-cost-to-go-term"><span>Preliminaries: Approximating the cost-to-go term</span></a></li><li><a class="tocitem" href="#Implementation:-modeling"><span>Implementation: modeling</span></a></li><li><a class="tocitem" href="#Implementation:-solution-algorithm"><span>Implementation: solution algorithm</span></a></li><li><a class="tocitem" href="#Implementation:-the-forward-pass"><span>Implementation: the forward pass</span></a></li><li><a class="tocitem" href="#Implementation:-the-backward-pass"><span>Implementation: the backward pass</span></a></li><li><a class="tocitem" href="#Implementation:-the-training-loop"><span>Implementation: the training loop</span></a></li><li><a class="tocitem" href="#Implementation:-decision-rules"><span>Implementation: decision rules</span></a></li><li><a class="tocitem" href="#Example:-infinite-horizon"><span>Example: infinite horizon</span></a></li></ul></li></ul></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox"/><label class="tocitem" for="menuitem-3"><span class="docs-label">How-to guides</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../guides/add_a_multidimensional_state_variable/">Add a multi-dimensional state variable</a></li><li><a class="tocitem" href="../../guides/add_a_risk_measure/">Add a risk measure</a></li><li><a class="tocitem" href="../../guides/add_multidimensional_noise_Terms/">Add multi-dimensional noise terms</a></li><li><a class="tocitem" href="../../guides/add_noise_in_the_constraint_matrix/">Add noise in the constraint matrix</a></li><li><a class="tocitem" href="../../guides/choose_a_stopping_rule/">Choose a stopping rule</a></li><li><a class="tocitem" href="../../guides/create_a_general_policy_graph/">Create a general policy graph</a></li><li><a class="tocitem" href="../../guides/debug_a_model/">Debug a model</a></li><li><a class="tocitem" href="../../guides/improve_computational_performance/">Improve computational performance</a></li><li><a class="tocitem" href="../../guides/simulate_using_a_different_sampling_scheme/">Simulate using a different sampling scheme</a></li><li><a class="tocitem" href="../../guides/upgrade_from_the_old_sddp/">Upgrade from the old SDDP.jl</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">Examples</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../examples/simple_hydro_thermal/">Hydro-thermal scheduling</a></li><li><a class="tocitem" href="../../examples/the_farmers_problem/">The farmer&#39;s problem</a></li></ul></li><li><a class="tocitem" href="../../apireference/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href>Expert I: Pedagogical SDDP</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Expert I: Pedagogical SDDP</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/odow/SDDP.jl/blob/master/docs/src/tutorial/22_pedagogical_sddp.jl" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Expert-I:-Pedagogical-SDDP"><a class="docs-heading-anchor" href="#Expert-I:-Pedagogical-SDDP">Expert I: Pedagogical SDDP</a><a id="Expert-I:-Pedagogical-SDDP-1"></a><a class="docs-heading-anchor-permalink" href="#Expert-I:-Pedagogical-SDDP" title="Permalink"></a></h1><p>In this tutorial we walk through a simplified implementation of stochastic dual dynamic programming to explain the key concepts.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>If you haven&#39;t already, go read <a href="../01_first_steps/#Basic-I:-first-steps">Basic I: first steps</a>, since it introduces much necessary background theory, and <a href="../02_adding_uncertainty/#Basic-II:-adding-uncertainty">Basic II: adding uncertainty</a>, since it defines the same example we will solve in our implementation.</p></div></div><p>For this implementation of SDDP, we&#39;re going to try and keep things as simple. This is very much a &quot;vanilla&quot; version of SDDP; it doesn&#39;t have (m)any fancy computational tricks that you need to code a performant or stable version that will work on realistic instances. However, it will work on arbitrary policy graphs, including those with cycles such as infinite horizon problems!</p><p>In the interests of brevity, we will also include minimal error checking. Think about all the different ways you could break this code!</p><p>This tutorial uses the following packages. For clarity, we call <code>import PackageName</code> so that we must prefix <code>PackageName.</code> to all functions and structs provided by that package. Everything not prefixed is either part of base Julia, or we wrote it.</p><pre><code class="language-julia">import ForwardDiff
import GLPK
import JuMP
import Statistics</code></pre><h2 id="Preliminaries:-Kelley&#39;s-cutting-plane-algorithm"><a class="docs-heading-anchor" href="#Preliminaries:-Kelley&#39;s-cutting-plane-algorithm">Preliminaries: Kelley&#39;s cutting plane algorithm</a><a id="Preliminaries:-Kelley&#39;s-cutting-plane-algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#Preliminaries:-Kelley&#39;s-cutting-plane-algorithm" title="Permalink"></a></h2><p>Kelley&#39;s cutting plane algorithm is an iterative method for minimizing convex functions. Given a convex function <span>$f(x)$</span>, Kelley&#39;s constructs an under-approximation of the function at the minimum by a set of first-order Taylor series approximations (called <strong>cuts</strong>) constructed at a set of <span>$K$</span> points <span>$k = 1,\ldots,K$</span>:</p><p class="math-container">\[\begin{aligned}
f^K = \min\limits_{\theta \in \mathbb{R}, x \in \mathbb{R}^N} \;\; &amp; \theta\\
&amp; \theta \ge f(x_k) + \frac{df}{dx}\left(x_k\right)^\top (x - x_k),\quad k=1,\ldots,K\\
&amp; \theta \ge M,
\end{aligned}\]</p><p>where <span>$M$</span> is a sufficiently large negative number that is a lower bound for <span>$f$</span> over the domain of <span>$x$</span>.</p><p>As more cuts are added:</p><p class="math-container">\[\lim_{K \rightarrow \infty} f^K = \min\limits_{x \in \mathbb{R}^N} f(x)\]</p><h3 id="Bounds"><a class="docs-heading-anchor" href="#Bounds">Bounds</a><a id="Bounds-1"></a><a class="docs-heading-anchor-permalink" href="#Bounds" title="Permalink"></a></h3><p>By convexity, <span>$f^K \le f(x)$</span> for all <span>$x$</span>. Thus, if <span>$x^*$</span> is a minimizer of <span>$f$</span>, then at any point in time we can construct a lower bound for <span>$f(x^*)$</span> by solving <span>$f^K$</span>.</p><p>Moreover, since any feasible point is an upper bound, we can use the primal solution <span>$x^K$</span> returned by solving <span>$f^K$</span> to evaluate <span>$f(x_K)$</span> to generate an upper bound.</p><p>Therefore, <span>$f^K \le f(x^*) \le f(x_K)$</span>.</p><h3 id="Implementation"><a class="docs-heading-anchor" href="#Implementation">Implementation</a><a id="Implementation-1"></a><a class="docs-heading-anchor-permalink" href="#Implementation" title="Permalink"></a></h3><p>Here is pseudo-code fo the Kelley algorithm:</p><ol><li>Take as input a function <span>$f$</span> and a iteration limit <span>$K_{max}$</span>. Set <span>$K = 0$</span>, and initialize <span>$f^K$</span>.</li><li>Solve <span>$f^K$</span> to obtain a candidate solution <span>$x_{K+1}$</span>.</li><li>Add a cut <span>$\theta \ge f(x_{K+1}) + \frac{df}{dx}\left(x_{K+1}\right)^\top (x - x_{K+1})$</span> to form <span>$f^{K+1}$</span>.</li><li>Increment <span>$K$</span></li><li>If <span>$K = K_{max}$</span>, STOP, otherwise, go to step 2.</li></ol><p>And here&#39;s a complete implementation:</p><pre><code class="language-julia">function kelleys_cutting_plane(
    # The function to be minimized.
    f::Function,
    # The gradient of `f`. By default, we use automatic differentiation to
    # compute the gradient of f so the user doesn&#39;t have to!
    dfdx::Function = x -&gt; ForwardDiff.gradient(f, x);
    # The number of arguments to `f`.
    input_dimension::Int,
    # A lower bound for the function `f` over its domain.
    lower_bound::Float64,
    # The number of iterations to run Kelley&#39;s algorithm for before stopping.
    iteration_limit::Int,
)
    # Step (1):
    K = 0
    model = JuMP.Model(GLPK.Optimizer)
    JuMP.@variable(model, θ &gt;= lower_bound)
    JuMP.@variable(model, x[1:input_dimension])
    JuMP.@objective(model, Min, θ)
    while true
        # Step (2)
        JuMP.optimize!(model)
        x_k = JuMP.value.(x)
        # Step (3):
        c = JuMP.@constraint(model, θ &gt;= f(x_k) + dfdx(x_k)&#39; * (x .- x_k))
        # Step (4):
        K = K + 1
        # Step (5):
        if K == iteration_limit
            break
        end
    end
    θ_K, x_K = JuMP.value(θ), JuMP.value.(x)
    println(&quot;Found solution:&quot;)
    println(&quot;  x_K   = &quot;, x_K)
    println(&quot;  f(x*) ∈ [&quot;, θ_K, &quot;, &quot;, f(x_K), &quot;]&quot;)
    return
end</code></pre><pre class="documenter-example-output">kelleys_cutting_plane (generic function with 2 methods)</pre><p>Let&#39;s run our algorithm to see what happens:</p><pre><code class="language-julia">kelleys_cutting_plane(
    input_dimension = 2,
    lower_bound = 0.0,
    iteration_limit = 20,
) do x
    return (x[1] - 1)^2 + (x[2] + 2)^2
end</code></pre><pre class="documenter-example-output">Found solution:
  x_K   = [0.997655, -2.00352]
  f(x*) ∈ [0.0, 1.7896182563008953e-5]</pre><h2 id="Preliminaries:-Approximating-the-cost-to-go-term"><a class="docs-heading-anchor" href="#Preliminaries:-Approximating-the-cost-to-go-term">Preliminaries: Approximating the cost-to-go term</a><a id="Preliminaries:-Approximating-the-cost-to-go-term-1"></a><a class="docs-heading-anchor-permalink" href="#Preliminaries:-Approximating-the-cost-to-go-term" title="Permalink"></a></h2><p>In <a href="../01_first_steps/#Basic-I:-first-steps">Basic I: first steps</a>, we discussed how you could formulate an optimal policy to a multistage stochastic program using the dynamic programming recursion:</p><p class="math-container">\[\begin{aligned}
V_i(x, \omega) = \min\limits_{\bar{x}, x^\prime, u} \;\; &amp; C_i(\bar{x}, u, \omega) + \mathbb{E}_{j \in i^+, \varphi \in \Omega_j}[V_j(x^\prime, \varphi)]\\
&amp; x^\prime = T_i(\bar{x}, u, \omega) \\
&amp; u \in U_i(\bar{x}, \omega) \\
&amp; \bar{x} = x
\end{aligned}\]</p><p>where our decision rule, <span>$\pi_i(x, \omega)$</span>, solves this optimization problem and returns a <span>$u^*$</span> corresponding to an optimal solution. Moreover, we alluded to the fact that the cost-to-go term (the nasty recursive expectation) makes this problem intractable to solve.</p><p>However, if, excluding the cost-to-go term, <span>$V_i(x, \omega)$</span> can be formulated as a linear program (this also works for convex programs, but the math is more involved), then we can make some progress.</p><p>First, notice that <span>$x$</span> only appears as a right-hand side term of <span>$V_i$</span>. Therefore, <span>$V_i(x, \cdot)$</span> is convex with respect to <span>$x$</span> for fixed <span>$\omega$</span>. Moreover, the reduced cost of the decision variable <span>$\bar{x}$</span> is a subgradient of the function <span>$V_i$</span> with respect to <span>$x$</span>! (This is one reason why we add the <span>$\bar{x}$</span> and the fishing constraint <span>$\bar{x} = x$</span>.)</p><p>Second, a convex combination of convex functions is also convex, so the cost-to-go term is a convex function of <span>$x^\prime$</span>.</p><p>Stochastic dual dynamic programming converts this problem into a tractable form by applying Kelley&#39;s cutting plane algorithm to the cost-to-go term:</p><p class="math-container">\[\begin{aligned}
V_i^K(x, \omega) = \min\limits_{\bar{x}, x^\prime, u} \;\; &amp; C_i(\bar{x}, u, \omega) + \theta\\
&amp; x^\prime = T_i(\bar{x}, u, \omega) \\
&amp; u \in U_i(\bar{x}, \omega) \\
&amp; \bar{x} = x \\
&amp; \theta \ge \mathbb{E}_{j \in i^+, \varphi \in \Omega_j}\left[V_j^k(x^\prime_k, \varphi) + \frac{dV_j^k}{dx^\prime}\left(x^\prime_k, \varphi\right)^\top (x^\prime - x^\prime_k)\right],\quad k=1,\ldots,K \\
&amp; \theta \ge M
\end{aligned}\]</p><p>All we need now is a way of generating these cutting planes in an iterative manner. Before we get to that though, let&#39;s start writing some code.</p><h2 id="Implementation:-modeling"><a class="docs-heading-anchor" href="#Implementation:-modeling">Implementation: modeling</a><a id="Implementation:-modeling-1"></a><a class="docs-heading-anchor-permalink" href="#Implementation:-modeling" title="Permalink"></a></h2><p>Let&#39;s make a start by defining the problem structure. Like SDDP.jl, we need a few things:</p><ol><li>A description of the structure of the policy graph: how many nodes there are, and the arcs linking the nodes together with their corresponding probabilities.</li><li>A JuMP model for each node in the policy graph</li><li>A way to identify the incoming and outgoing state variables of each node</li><li>A description of the random variable, as well as a function that we can call that will modify the JuMP model to reflect the realization of the random variable.</li><li>A decision variable to act as the approximated cost-to-go term.</li></ol><h3 id="Structs"><a class="docs-heading-anchor" href="#Structs">Structs</a><a id="Structs-1"></a><a class="docs-heading-anchor-permalink" href="#Structs" title="Permalink"></a></h3><p>The first struct we are going to use is a <code>State</code> struct that will wrap an incoming and outgoing state variable.</p><pre><code class="language-julia">struct State
    in::JuMP.VariableRef
    out::JuMP.VariableRef
end</code></pre><p>Next, we need a struct to wrap all of the uncertainty within a node.</p><pre><code class="language-julia">struct Uncertainty
    parameterize::Function
    Ω::Vector{Any}
    P::Vector{Float64}
end</code></pre><p><code>parameterize</code> is a function, which takes a realization of the random variable <span>$\omega\in\Omega$</span> and updates the subproblem accordingly. The finite discrete random variable is defined by the vectors <code>Ω</code> and <code>P</code>, so that the random variable takes the value <code>Ω[i]</code> with probability <code>P[i]</code>. As such, <code>P</code> should sum to 1. (We don&#39;t check this here, but we should; we do in SDDP.jl.)</p><p>Now we have two building blocks, we can declare the structure of each node.</p><pre><code class="language-julia">struct Node
    subproblem::JuMP.Model
    states::Dict{Symbol,State}
    uncertainty::Uncertainty
    cost_to_go::JuMP.VariableRef
end</code></pre><ul><li><code>subproblem</code> is going to be the JuMP model that we build at each node.</li><li><code>states</code> is a dictionary that maps a symbolic name of a state variable to a <code>State</code> object wrapping the incoming and outgoing state variables in <code>subproblem</code>.</li><li><code>uncertainty</code> is an <code>Uncertainty</code> object described above.</li><li><code>cost_to_go</code> is a JuMP variable that approximates the cost-to-go term.</li></ul><p>Finally, we define a simplified policy graph as follows:</p><pre><code class="language-julia">struct PolicyGraph
    nodes::Vector{Node}
    arcs::Vector{Dict{Int,Float64}}
end</code></pre><p>There is a vector of nodes, as well as a data structure for the arcs. <code>arcs</code> is a vector of dictionaries, where <code>arcs[i][j]</code> gives the probabiltiy of transitioning from node <code>i</code> to node <code>j</code>, if an arc exists.</p><p>To simplify things, we will assume that the root node transitions to node <code>1</code> with probability 1, and there are no other incoming arcs to node 1. Notably, we can still define cyclic graphs though!</p><p>We also define a nice <code>show</code> method so that we don&#39;t accidentally print a large amount of information to the screen when creating a model.</p><pre><code class="language-julia">function Base.show(io::IO, model::PolicyGraph)
    println(io, &quot;A policy graph with $(length(model.nodes)) nodes&quot;)
    println(io, &quot;Arcs:&quot;)
    for (from, arcs) in enumerate(model.arcs)
        for (to, probability) in arcs
            println(io, &quot;  $(from) =&gt; $(to) w.p. $(probability)&quot;)
        end
    end
    return
end</code></pre><h3 id="Functions"><a class="docs-heading-anchor" href="#Functions">Functions</a><a id="Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Functions" title="Permalink"></a></h3><p>Now we have some basic types, let&#39;s implement some functions so that the user can create a model.</p><p>First, we need an example of a function that the user will provide. Like SDDP.jl, this takes an empty <code>subproblem</code>, and a node index, in this case <code>t::Int</code>. You could change this function to change the model, or define a new one later in the code.</p><pre><code class="language-julia">function subproblem_builder(subproblem::JuMP.Model, t::Int)
    # Define the state variables. Note how we fix the incoming state to the
    # initial state variable regardless of `t`! This isn&#39;t strictly necessary;
    # it only matters that we do it for the first node.
    JuMP.@variable(subproblem, volume_in == 200)
    JuMP.@variable(subproblem, 0 &lt;= volume_out &lt;= 200)
    states = Dict(:volume =&gt; State(volume_in, volume_out))
    # Define the control variables.
    JuMP.@variables(subproblem, begin
        thermal_generation &gt;= 0
        hydro_generation   &gt;= 0
        hydro_spill        &gt;= 0
        inflow
    end)
    # Define the constraints
    JuMP.@constraints(subproblem, begin
        volume_out == volume_in + inflow - hydro_generation - hydro_spill
        demand_constraint, thermal_generation + hydro_generation == 150.0
    end)
    # Define the objective for each stage `t`. Note that we can use `t` as an
    # index for t = 1, 2, 3.
    fuel_cost = [50.0, 100.0, 150.0]
    JuMP.@objective(subproblem, Min, fuel_cost[t] * thermal_generation)
    # Finally, we define the uncertainty object. Because this is a simplified
    # implementation of SDDP, we shall politely ask the user to only modify the
    # constraints, and not the objective function! (Not that it changes the
    # algorithm, we just have to add more information to keep track of things.)
    uncertainty = Uncertainty([0.0, 50.0, 100.0], [1 / 3, 1 / 3, 1 / 3]) do ω
        JuMP.fix(inflow, ω)
    end
    return states, uncertainty
end</code></pre><pre class="documenter-example-output">subproblem_builder (generic function with 1 method)</pre><p>If you&#39;ve read <a href="../02_adding_uncertainty/#Basic-II:-adding-uncertainty">Basic II: adding uncertainty</a>, this example should be familiar. You can probably see how some of the SDDP.jl functionality like <a href="../../apireference/#SDDP.@stageobjective"><code>@stageobjective</code></a> and <a href="../../apireference/#SDDP.parameterize"><code>SDDP.parameterize</code></a> help smooth some of the usability issues like needing to construct both the incoming and outgoing state variables, or needing to explicitly <code>return states, uncertainty</code>.</p><p>The next function we need to define is the analog of <a href="../../apireference/#SDDP.PolicyGraph"><code>SDDP.PolicyGraph</code></a>. It should be pretty readable.</p><pre><code class="language-julia">function PolicyGraph(
    subproblem_builder::Function;
    graph::Vector{Dict{Int,Float64}},
    lower_bound::Float64,
    optimizer,
)
    nodes = Node[]
    for t = 1:length(graph)
        # Create a model.
        model = JuMP.Model(optimizer)
        # Use the provided function to build out each subproblem. The user&#39;s
        # function returns a dictionary mapping `Symbol`s to `State` objects,
        # and an `Uncertainty` object.
        states, uncertainty = subproblem_builder(model, t)
        # Now add the cost-to-go terms:
        JuMP.@variable(model, cost_to_go &gt;= lower_bound)
        obj = JuMP.objective_function(model)
        JuMP.@objective(model, Min, obj + cost_to_go)
        # If there are no outgoing arcs, the cost-to-go is 0.0.
        if length(graph[t]) == 0
            JuMP.fix(cost_to_go, 0.0; force = true)
        end
        push!(nodes, Node(model, states, uncertainty, cost_to_go))
    end
    return PolicyGraph(nodes, graph)
end</code></pre><pre class="documenter-example-output">Main.ex-22_pedagogical_sddp.PolicyGraph</pre><p>Then, we can create a model using the <code>subproblem_builder</code> function we defined earlier:</p><pre><code class="language-julia">model = PolicyGraph(
    subproblem_builder;
    graph = [
        Dict(2 =&gt; 1.0),
        Dict(3 =&gt; 1.0),
        Dict{Int,Float64}(),
    ],
    lower_bound = 0.0,
    optimizer = GLPK.Optimizer,
)</code></pre><pre class="documenter-example-output">A policy graph with 3 nodes
Arcs:
  1 =&gt; 2 w.p. 1.0
  2 =&gt; 3 w.p. 1.0
</pre><h2 id="Implementation:-solution-algorithm"><a class="docs-heading-anchor" href="#Implementation:-solution-algorithm">Implementation: solution algorithm</a><a id="Implementation:-solution-algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#Implementation:-solution-algorithm" title="Permalink"></a></h2><p>Before we get properly coding the solution algorithm, it&#39;s also going to be useful to have a function that samples a realization of the random variable defined by <code>Ω</code> and <code>P</code>:</p><pre><code class="language-julia">function sample_uncertainty(uncertainty::Uncertainty)
    r = rand()
    for (p, ω) in zip(uncertainty.P, uncertainty.Ω)
        r -= p
        if r &lt; 0.0
            return ω
        end
    end
    error(&quot;We should never get here because P should sum to 1.0.&quot;)
end</code></pre><pre class="documenter-example-output">sample_uncertainty (generic function with 1 method)</pre><p>You should be able to work out what is going on. <code>rand()</code> samples a uniform random variable in <code>[0, 1)</code>. For example:</p><pre><code class="language-julia">sample_uncertainty(model.nodes[1].uncertainty)</code></pre><pre class="documenter-example-output">100.0</pre><p>It&#39;s also going to be useful to define a function that generates a random walk through the nodes of the graph:</p><pre><code class="language-julia">function sample_graph(model::PolicyGraph)
    trajectory, current_node = Int[], 1
    finished = false
    while !finished
        push!(trajectory, current_node)
        r = rand()
        for (to, probability) in model.arcs[current_node]
            r -= probability
            if r &lt; 0.0
                current_node = to
                break
            end
        end
        if r &gt;= 0
            # We looped through the outgoing arcs and still have probability
            # left over! This means it&#39;s time to stop walking.
            finished = true
        end
    end
    return trajectory
end</code></pre><pre class="documenter-example-output">sample_graph (generic function with 1 method)</pre><p>For example:</p><pre><code class="language-julia">sample_graph(model)</code></pre><pre class="documenter-example-output">3-element Array{Int64,1}:
 1
 2
 3</pre><p>This is a little boring, because our graph is simple. However, more complicated graphs will generate more interesting trajectories!</p><p>A third function that is going to be useful is a way to compute a lower bound for the objective of the policy graph:</p><pre><code class="language-julia">function lower_bound(model::PolicyGraph)
    node = model.nodes[1]
    bound = 0.0
    for (p, ω) in zip(node.uncertainty.P, node.uncertainty.Ω)
        node.uncertainty.parameterize(ω)
        JuMP.optimize!(node.subproblem)
        bound += p * JuMP.objective_value(node.subproblem)
    end
    return bound
end</code></pre><pre class="documenter-example-output">lower_bound (generic function with 1 method)</pre><p>Because we haven&#39;t trained a policy yet, the lower bound is going to be very bad:</p><pre><code class="language-julia">lower_bound(model)</code></pre><pre class="documenter-example-output">0.0</pre><h2 id="Implementation:-the-forward-pass"><a class="docs-heading-anchor" href="#Implementation:-the-forward-pass">Implementation: the forward pass</a><a id="Implementation:-the-forward-pass-1"></a><a class="docs-heading-anchor-permalink" href="#Implementation:-the-forward-pass" title="Permalink"></a></h2><p>The forward pass walks the policy graph from start to end, and solves each approximated subproblem to generate a candidate outgoing state variable <span>$x_k^\prime$</span> at which to generate a cut.</p><p>It takes a <code>::PolicyGraph</code>, and returns a tuple of two things: a vector of the outgoing state variables visited, and a <code>Float64</code> of the cumulative stage costs that were incurred along the forward pass.</p><pre><code class="language-julia">function forward_pass(model::PolicyGraph, io::IO = stdout)
    println(io, &quot;| Forward Pass&quot;)
    # First, get the value of the state at the root node (e.g., x_R).
    incoming_state = Dict(
        k =&gt; JuMP.fix_value(v.in) for (k, v) in model.nodes[1].states
    )
    # `simulation_cost` is an accumlator that is going to sum the stage-costs
    # incurred over the forward pass.
    simulation_cost = 0.0
    # We also need to record the nodes visited and resultant outgoing state
    # variables so we can pass them to the backward pass.
    trajectory = Tuple{Int,Dict{Symbol,Float64}}[]
    # Now&#39;s the meat of the forward pass: loop through each of the nodes
    for t in sample_graph(model)
        node = model.nodes[t]
        println(io, &quot;| | Visiting node $(t)&quot;)
        # Sample the uncertainty:
        ω = sample_uncertainty(node.uncertainty)
        println(io, &quot;| |  ω = &quot;, ω)
        # Before parameterizing the subproblem using the user-provided
        # function:
        node.uncertainty.parameterize(ω)
        println(io, &quot;| |  x = &quot;, incoming_state)
        # Update the incoming state variable:
        for (k, v) in incoming_state
            JuMP.fix(node.states[k].in, v; force = true)
        end
        # Now solve the subproblem and check we found an optimal solution:
        JuMP.optimize!(node.subproblem)
        if JuMP.termination_status(node.subproblem) != JuMP.MOI.OPTIMAL
            error(&quot;Something went terribly wrong!&quot;)
        end
        # Compute the outgoing state variables:
        outgoing_state = Dict(k =&gt; JuMP.value(v.out) for (k, v) in node.states)
        println(io, &quot;| |  x′ = &quot;, outgoing_state)
        # We also need to compute the stage cost to add to our
        # `simulation_cost` accumulator:
        stage_cost = JuMP.objective_value(node.subproblem) - JuMP.value(node.cost_to_go)
        simulation_cost += stage_cost
        println(io, &quot;| |  C(x, u, ω) = &quot;, stage_cost)
        # As a final step, set the outgoing state of stage t and the incoming
        # state of stage t + 1, and add the node to the trajectory.
        incoming_state = outgoing_state
        push!(trajectory, (t, outgoing_state))
    end
    return trajectory, simulation_cost
end</code></pre><pre class="documenter-example-output">forward_pass (generic function with 2 methods)</pre><h2 id="Implementation:-the-backward-pass"><a class="docs-heading-anchor" href="#Implementation:-the-backward-pass">Implementation: the backward pass</a><a id="Implementation:-the-backward-pass-1"></a><a class="docs-heading-anchor-permalink" href="#Implementation:-the-backward-pass" title="Permalink"></a></h2><p>We&#39;re now ready to code the backward pass. This is going to take a <code>::PolicyGraph</code> object, and a vector of (node, outgoing states) tuples from the forward pass. It returns a <code>Float64</code> that is a valid lower bound for the objective of the multistage stochastic program.</p><pre><code class="language-julia">function backward_pass(
    model::PolicyGraph,
    trajectory::Vector{Tuple{Int,Dict{Symbol,Float64}}},
    io::IO = stdout,
)
    println(io, &quot;| Backward pass&quot;)
    # For the backward pass, we walk back up the nodes, from the final
    # node to the second (we will solve the first node after this loop).
    for i = reverse(1:length(trajectory)-1)
        index, outgoing_states = trajectory[i]
        node = model.nodes[index]
        println(io, &quot;| | Visiting node $(index)&quot;)
        # Create an empty affine expression that we will use to build up the
        # cut expression.
        cut_expression = JuMP.AffExpr(0.0)
        # Now for each possible node and realization of the uncertainty, solve
        # the subproblem, and add `P_ij * p_ω * [y + λᵀ(x - x_k)]` to the cut
        # expression. (See the Theory section above is this isn&#39;t obvious why.)
        for (j, P_ij) in model.arcs[index]
            next_node = model.nodes[j]
            for (k, v) in outgoing_states
                JuMP.fix(next_node.states[k].in, v; force = true)
            end
            for (pω, ω) in zip(next_node.uncertainty.P, next_node.uncertainty.Ω)
                println(io, &quot;| | | Solving ω = &quot;, ω)
                next_node.uncertainty.parameterize(ω)
                JuMP.optimize!(next_node.subproblem)
                y = JuMP.objective_value(next_node.subproblem)
                println(io, &quot;| | |  y = &quot;, y)
                λ = Dict(k =&gt; JuMP.reduced_cost(v.in) for (k, v) in next_node.states)
                println(io, &quot;| | |  λ = &quot;, λ)
                cut_expression += P_ij * pω * JuMP.@expression(
                    node.subproblem,
                    y + sum(
                        λ[k] * (x.out - outgoing_states[k])
                        for (k, x) in node.states
                    ),
                )
            end
        end
        # And then refine the cost-to-go variable by adding a cut that is the
        # expectation of the cuts computed in the step above.
        c = JuMP.@constraint(
            node.subproblem, node.cost_to_go &gt;= cut_expression
        )
        println(io, &quot;| | | Adding cut : &quot;, c)
    end
    # Finally, compute a lower bound for the problem by evaluating the
    # first-stage subproblem.
    return lower_bound(model)
end</code></pre><pre class="documenter-example-output">backward_pass (generic function with 2 methods)</pre><h2 id="Implementation:-the-training-loop"><a class="docs-heading-anchor" href="#Implementation:-the-training-loop">Implementation: the training loop</a><a id="Implementation:-the-training-loop-1"></a><a class="docs-heading-anchor-permalink" href="#Implementation:-the-training-loop" title="Permalink"></a></h2><p>The <code>train</code> loop of SDDP just applies the forward and backward passes iteratively, followed by a final simulation to compute the upper bound confidence interval.</p><p>First, we need a function to simulate the policy. This is going be very simple. It doesn&#39;t have an bells and whistles like being able to record the control variables. The confidence interval is also incorrect if there are cycles in the graph, because the distribution of simulation costs <code>z</code> is not symmetric.</p><pre><code class="language-julia">function simulate(model::PolicyGraph, io::IO = stdout; replications::Int)
    # Pipe the output to `devnull` so we don&#39;t print too much!
    simulations = [forward_pass(model, devnull) for _ = 1:replications]
    z = [s[2] for s in simulations]
    μ  = Statistics.mean(z)
    tσ = 1.96 * Statistics.std(z) / sqrt(replications)
    println(io, &quot;Upper bound = $(μ) ± $(tσ)&quot;)
    return simulations
end</code></pre><pre class="documenter-example-output">simulate (generic function with 2 methods)</pre><p>Here&#39;s the actual training loop:</p><pre><code class="language-julia">function train(
    model::PolicyGraph;
    iteration_limit::Int,
    replications::Int,
    io::IO = stdout,
)
    for i = 1:iteration_limit
        println(io, &quot;Starting iteration $(i)&quot;)
        outgoing_states, simulation = forward_pass(model, io)
        lower_bound = backward_pass(model, outgoing_states, io)
        println(io, &quot;| Finished iteration&quot;)
        println(io, &quot;| | simulation = &quot;, simulation)
        println(io, &quot;| | lower_bound = &quot;, lower_bound)
    end
    simulate(model, io; replications = replications)
    return
end</code></pre><pre class="documenter-example-output">train (generic function with 1 method)</pre><p>Using our <code>model</code> we defined earlier, we can go:</p><pre><code class="language-julia">train(model; iteration_limit = 3, replications = 100)</code></pre><pre class="documenter-example-output">Starting iteration 1
| Forward Pass
| | Visiting node 1
| |  ω = 50.0
| |  x = Dict(:volume=&gt;200.0)
| |  x′ = Dict(:volume=&gt;100.0)
| |  C(x, u, ω) = 0.0
| | Visiting node 2
| |  ω = 50.0
| |  x = Dict(:volume=&gt;100.0)
| |  x′ = Dict(:volume=&gt;0.0)
| |  C(x, u, ω) = 0.0
| | Visiting node 3
| |  ω = 50.0
| |  x = Dict(:volume=&gt;0.0)
| |  x′ = Dict(:volume=&gt;0.0)
| |  C(x, u, ω) = 15000.0
| Backward pass
| | Visiting node 2
| | | Solving ω = 0.0
| | |  y = 22500.0
| | |  λ = Dict(:volume=&gt;-150.0)
| | | Solving ω = 50.0
| | |  y = 15000.0
| | |  λ = Dict(:volume=&gt;-150.0)
| | | Solving ω = 100.0
| | |  y = 7500.0
| | |  λ = Dict(:volume=&gt;-150.0)
| | | Adding cut : 150 volume_out + cost_to_go ≥ 15000.0
| | Visiting node 1
| | | Solving ω = 0.0
| | |  y = 15000.0
| | |  λ = Dict(:volume=&gt;-100.0)
| | | Solving ω = 50.0
| | |  y = 10000.0
| | |  λ = Dict(:volume=&gt;-100.0)
| | | Solving ω = 100.0
| | |  y = 5000.0
| | |  λ = Dict(:volume=&gt;-100.0)
| | | Adding cut : 99.99999999999999 volume_out + cost_to_go ≥ 20000.0
| Finished iteration
| | simulation = 15000.0
| | lower_bound = 5000.000000000002
Starting iteration 2
| Forward Pass
| | Visiting node 1
| |  ω = 50.0
| |  x = Dict(:volume=&gt;200.0)
| |  x′ = Dict(:volume=&gt;200.0)
| |  C(x, u, ω) = 5000.000000000002
| | Visiting node 2
| |  ω = 50.0
| |  x = Dict(:volume=&gt;200.0)
| |  x′ = Dict(:volume=&gt;100.0)
| |  C(x, u, ω) = -2.8421709430404007e-12
| | Visiting node 3
| |  ω = 50.0
| |  x = Dict(:volume=&gt;100.0)
| |  x′ = Dict(:volume=&gt;0.0)
| |  C(x, u, ω) = 0.0
| Backward pass
| | Visiting node 2
| | | Solving ω = 0.0
| | |  y = 7500.0
| | |  λ = Dict(:volume=&gt;-150.0)
| | | Solving ω = 50.0
| | |  y = 0.0
| | |  λ = Dict(:volume=&gt;-150.0)
| | | Solving ω = 100.0
| | |  y = 0.0
| | |  λ = Dict(:volume=&gt;0.0)
| | | Adding cut : 100 volume_out + cost_to_go ≥ 12500.0
| | Visiting node 1
| | | Solving ω = 0.0
| | |  y = 7499.999999999997
| | |  λ = Dict(:volume=&gt;-100.0)
| | | Solving ω = 50.0
| | |  y = 2499.9999999999973
| | |  λ = Dict(:volume=&gt;-100.0)
| | | Solving ω = 100.0
| | |  y = 0.0
| | |  λ = Dict(:volume=&gt;0.0)
| | | Adding cut : 66.66666666666666 volume_out + cost_to_go ≥ 16666.666666666664
| Finished iteration
| | simulation = 4999.999999999999
| | lower_bound = 8333.333333333332
Starting iteration 3
| Forward Pass
| | Visiting node 1
| |  ω = 50.0
| |  x = Dict(:volume=&gt;200.0)
| |  x′ = Dict(:volume=&gt;200.0)
| |  C(x, u, ω) = 5000.0
| | Visiting node 2
| |  ω = 50.0
| |  x = Dict(:volume=&gt;200.0)
| |  x′ = Dict(:volume=&gt;125.0)
| |  C(x, u, ω) = 2500.0
| | Visiting node 3
| |  ω = 50.0
| |  x = Dict(:volume=&gt;125.0)
| |  x′ = Dict(:volume=&gt;25.0)
| |  C(x, u, ω) = 0.0
| Backward pass
| | Visiting node 2
| | | Solving ω = 0.0
| | |  y = 3750.0
| | |  λ = Dict(:volume=&gt;-150.0)
| | | Solving ω = 50.0
| | |  y = 0.0
| | |  λ = Dict(:volume=&gt;0.0)
| | | Solving ω = 100.0
| | |  y = 0.0
| | |  λ = Dict(:volume=&gt;0.0)
| | | Adding cut : 50 volume_out + cost_to_go ≥ 7500.0
| | Visiting node 1
| | | Solving ω = 0.0
| | |  y = 7500.0
| | |  λ = Dict(:volume=&gt;-100.0)
| | | Solving ω = 50.0
| | |  y = 2500.0
| | |  λ = Dict(:volume=&gt;-100.0)
| | | Solving ω = 100.0
| | |  y = 0.0
| | |  λ = Dict(:volume=&gt;0.0)
| | | Adding cut : 66.66666666666666 volume_out + cost_to_go ≥ 16666.666666666664
| Finished iteration
| | simulation = 7500.0
| | lower_bound = 8333.333333333332
Upper bound = 8350.0 ± 950.4004684894944</pre><p>Success! We trained a policy for a finite horizon multistage stochastic program using stochastic dual dynamic programming.</p><h2 id="Implementation:-decision-rules"><a class="docs-heading-anchor" href="#Implementation:-decision-rules">Implementation: decision rules</a><a id="Implementation:-decision-rules-1"></a><a class="docs-heading-anchor-permalink" href="#Implementation:-decision-rules" title="Permalink"></a></h2><p>A final step is the ability to evaluate the decision rule associated with a node without having to perform a full simulation.</p><pre><code class="language-julia">function decision_rule(
    node::Node;
    incoming_state::Dict{Symbol,Float64},
    random_variable,
)
    node.uncertainty.parameterize(random_variable)
    for (k, v) in incoming_state
        JuMP.fix(node.states[k].in, v; force = true)
    end
    JuMP.optimize!(node.subproblem)
    return Dict(
        k =&gt; JuMP.value.(v)
        for (k, v) in JuMP.object_dictionary(node.subproblem)
    )
end

decision_rule(
    model.nodes[1];
    incoming_state = Dict(:volume =&gt; 150.0),
    random_variable = 75,
)</code></pre><pre class="documenter-example-output">Dict{Symbol,Float64} with 8 entries:
  :volume_out         =&gt; 200.0
  :demand_constraint  =&gt; 150.0
  :hydro_spill        =&gt; 0.0
  :inflow             =&gt; 75.0
  :volume_in          =&gt; 150.0
  :thermal_generation =&gt; 125.0
  :hydro_generation   =&gt; 25.0
  :cost_to_go         =&gt; 3333.33</pre><p>Note how the random variable can be <strong>out-of-sample</strong>, i.e., it doesn&#39;t have to be in the vector <span>$\Omega$</span> we created when defining the model! This is a notable difference to other multistage stochastic solution methods like progressive hedging or using the deterministic equivalent.</p><h2 id="Example:-infinite-horizon"><a class="docs-heading-anchor" href="#Example:-infinite-horizon">Example: infinite horizon</a><a id="Example:-infinite-horizon-1"></a><a class="docs-heading-anchor-permalink" href="#Example:-infinite-horizon" title="Permalink"></a></h2><p>First, create the model using the <code>subproblem_builder</code> function we defined earlier:</p><pre><code class="language-julia">model = PolicyGraph(
    subproblem_builder;
    graph = [
        Dict(2 =&gt; 1.0),
        Dict(3 =&gt; 1.0),
        Dict(2 =&gt; 0.5),
    ],
    lower_bound = 0.0,
    optimizer = GLPK.Optimizer,
)</code></pre><pre class="documenter-example-output">A policy graph with 3 nodes
Arcs:
  1 =&gt; 2 w.p. 1.0
  2 =&gt; 3 w.p. 1.0
  3 =&gt; 2 w.p. 0.5
</pre><p>Then, train a policy:</p><pre><code class="language-julia">train(model; iteration_limit = 3, replications = 100)</code></pre><pre class="documenter-example-output">Starting iteration 1
| Forward Pass
| | Visiting node 1
| |  ω = 50.0
| |  x = Dict(:volume=&gt;200.0)
| |  x′ = Dict(:volume=&gt;100.0)
| |  C(x, u, ω) = 0.0
| | Visiting node 2
| |  ω = 50.0
| |  x = Dict(:volume=&gt;100.0)
| |  x′ = Dict(:volume=&gt;0.0)
| |  C(x, u, ω) = 0.0
| | Visiting node 3
| |  ω = 100.0
| |  x = Dict(:volume=&gt;0.0)
| |  x′ = Dict(:volume=&gt;0.0)
| |  C(x, u, ω) = 7500.0
| Backward pass
| | Visiting node 2
| | | Solving ω = 0.0
| | |  y = 22500.0
| | |  λ = Dict(:volume=&gt;-150.0)
| | | Solving ω = 50.0
| | |  y = 15000.0
| | |  λ = Dict(:volume=&gt;-150.0)
| | | Solving ω = 100.0
| | |  y = 7500.0
| | |  λ = Dict(:volume=&gt;-150.0)
| | | Adding cut : 150 volume_out + cost_to_go ≥ 15000.0
| | Visiting node 1
| | | Solving ω = 0.0
| | |  y = 15000.0
| | |  λ = Dict(:volume=&gt;-100.0)
| | | Solving ω = 50.0
| | |  y = 10000.0
| | |  λ = Dict(:volume=&gt;-100.0)
| | | Solving ω = 100.0
| | |  y = 5000.0
| | |  λ = Dict(:volume=&gt;-100.0)
| | | Adding cut : 99.99999999999999 volume_out + cost_to_go ≥ 20000.0
| Finished iteration
| | simulation = 7500.0
| | lower_bound = 5000.000000000002
Starting iteration 2
| Forward Pass
| | Visiting node 1
| |  ω = 0.0
| |  x = Dict(:volume=&gt;200.0)
| |  x′ = Dict(:volume=&gt;200.0)
| |  C(x, u, ω) = 7500.000000000002
| | Visiting node 2
| |  ω = 0.0
| |  x = Dict(:volume=&gt;200.0)
| |  x′ = Dict(:volume=&gt;100.0)
| |  C(x, u, ω) = 4999.999999999997
| | Visiting node 3
| |  ω = 100.0
| |  x = Dict(:volume=&gt;100.0)
| |  x′ = Dict(:volume=&gt;50.0)
| |  C(x, u, ω) = 0.0
| | Visiting node 2
| |  ω = 50.0
| |  x = Dict(:volume=&gt;50.0)
| |  x′ = Dict(:volume=&gt;100.0)
| |  C(x, u, ω) = 15000.0
| | Visiting node 3
| |  ω = 50.0
| |  x = Dict(:volume=&gt;100.0)
| |  x′ = Dict(:volume=&gt;0.0)
| |  C(x, u, ω) = 0.0
| Backward pass
| | Visiting node 2
| | | Solving ω = 0.0
| | |  y = 7500.0
| | |  λ = Dict(:volume=&gt;-150.0)
| | | Solving ω = 50.0
| | |  y = 0.0
| | |  λ = Dict(:volume=&gt;-150.0)
| | | Solving ω = 100.0
| | |  y = 0.0
| | |  λ = Dict(:volume=&gt;0.0)
| | | Adding cut : 100 volume_out + cost_to_go ≥ 12500.0
| | Visiting node 3
| | | Solving ω = 0.0
| | |  y = 22500.0
| | |  λ = Dict(:volume=&gt;-100.0)
| | | Solving ω = 50.0
| | |  y = 17500.0
| | |  λ = Dict(:volume=&gt;-100.0)
| | | Solving ω = 100.0
| | |  y = 12500.0
| | |  λ = Dict(:volume=&gt;-100.0)
| | | Adding cut : 49.99999999999999 volume_out + cost_to_go ≥ 11249.999999999998
| | Visiting node 2
| | | Solving ω = 0.0
| | |  y = 18750.0
| | |  λ = Dict(:volume=&gt;-150.0)
| | | Solving ω = 50.0
| | |  y = 11249.999999999998
| | |  λ = Dict(:volume=&gt;-150.0)
| | | Solving ω = 100.0
| | |  y = 8749.999999999998
| | |  λ = Dict(:volume=&gt;-50.0)
| | | Adding cut : 116.66666666666666 volume_out + cost_to_go ≥ 24583.333333333332
| | Visiting node 1
| | | Solving ω = 0.0
| | |  y = 16250.000000000007
| | |  λ = Dict(:volume=&gt;-100.0)
| | | Solving ω = 50.0
| | |  y = 11249.999999999995
| | |  λ = Dict(:volume=&gt;-100.0)
| | | Solving ω = 100.0
| | |  y = 6250.000000000008
| | |  λ = Dict(:volume=&gt;-100.0)
| | | Adding cut : 99.99999999999999 volume_out + cost_to_go ≥ 31250.000000000007
| Finished iteration
| | simulation = 27500.0
| | lower_bound = 16250.000000000011
Starting iteration 3
| Forward Pass
| | Visiting node 1
| |  ω = 100.0
| |  x = Dict(:volume=&gt;200.0)
| |  x′ = Dict(:volume=&gt;200.0)
| |  C(x, u, ω) = 2500.0
| | Visiting node 2
| |  ω = 0.0
| |  x = Dict(:volume=&gt;200.0)
| |  x′ = Dict(:volume=&gt;200.0)
| |  C(x, u, ω) = 15000.000000000005
| | Visiting node 3
| |  ω = 0.0
| |  x = Dict(:volume=&gt;200.0)
| |  x′ = Dict(:volume=&gt;50.0)
| |  C(x, u, ω) = 0.0
| | Visiting node 2
| |  ω = 50.0
| |  x = Dict(:volume=&gt;50.0)
| |  x′ = Dict(:volume=&gt;100.0)
| |  C(x, u, ω) = 15000.000000000004
| | Visiting node 3
| |  ω = 50.0
| |  x = Dict(:volume=&gt;100.0)
| |  x′ = Dict(:volume=&gt;0.0)
| |  C(x, u, ω) = 0.0
| Backward pass
| | Visiting node 2
| | | Solving ω = 0.0
| | |  y = 18750.0
| | |  λ = Dict(:volume=&gt;-150.0)
| | | Solving ω = 50.0
| | |  y = 11249.999999999998
| | |  λ = Dict(:volume=&gt;-150.0)
| | | Solving ω = 100.0
| | |  y = 8749.999999999998
| | |  λ = Dict(:volume=&gt;-50.0)
| | | Adding cut : 116.66666666666666 volume_out + cost_to_go ≥ 24583.333333333332
| | Visiting node 3
| | | Solving ω = 0.0
| | |  y = 33750.0
| | |  λ = Dict(:volume=&gt;-116.667)
| | | Solving ω = 50.0
| | |  y = 27916.666666666664
| | |  λ = Dict(:volume=&gt;-116.667)
| | | Solving ω = 100.0
| | |  y = 22083.333333333332
| | |  λ = Dict(:volume=&gt;-116.667)
| | | Adding cut : 58.33333333333333 volume_out + cost_to_go ≥ 16875.0
| | Visiting node 2
| | | Solving ω = 0.0
| | |  y = 13958.333333333332
| | |  λ = Dict(:volume=&gt;-58.3333)
| | | Solving ω = 50.0
| | |  y = 11041.666666666668
| | |  λ = Dict(:volume=&gt;-58.3333)
| | | Solving ω = 100.0
| | |  y = 8125.000000000001
| | |  λ = Dict(:volume=&gt;-58.3333)
| | | Adding cut : 58.33333333333333 volume_out + cost_to_go ≥ 22708.333333333336
| | Visiting node 1
| | | Solving ω = 0.0
| | |  y = 19791.666666666668
| | |  λ = Dict(:volume=&gt;-58.3333)
| | | Solving ω = 50.0
| | |  y = 16875.0
| | |  λ = Dict(:volume=&gt;-58.3333)
| | | Solving ω = 100.0
| | |  y = 13958.333333333336
| | |  λ = Dict(:volume=&gt;-58.3333)
| | | Adding cut : 58.33333333333333 volume_out + cost_to_go ≥ 28541.666666666664
| Finished iteration
| | simulation = 32500.00000000001
| | lower_bound = 21875.0
Upper bound = 30496.428571428565 ± 5961.585885712566</pre><p>Success! We trained a policy for an infinite horizon multistage stochastic program using stochastic dual dynamic programming. Note how some of the forward passes are different lengths!</p><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../13_integrality/">« Advanced III: integrality</a><a class="docs-footer-nextpage" href="../../guides/add_a_multidimensional_state_variable/">Add a multi-dimensional state variable »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Thursday 7 January 2021 01:12">Thursday 7 January 2021</span>. Using Julia version 1.0.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
